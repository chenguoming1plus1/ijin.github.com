<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[@ijin]]></title>
  <link href="http://ijin.github.io/atom.xml" rel="self"/>
  <link href="http://ijin.github.io/"/>
  <updated>2015-05-01T12:54:24+09:00</updated>
  <id>http://ijin.github.io/</id>
  <author>
    <name><![CDATA[Michael H. Oshita]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[EC2 Auto Recoveryの注意点]]></title>
    <link href="http://ijin.github.io/blog/2015/05/01/notes-on-ec2-auto-recovery/"/>
    <updated>2015-05-01T11:45:00+09:00</updated>
    <id>http://ijin.github.io/blog/2015/05/01/notes-on-ec2-auto-recovery</id>
    <content type="html"><![CDATA[<p>先日、EC2のAuto Recoveryでちょっとハマったのでメモ。</p>

<p>Cloudwatchの<code>StatusCheckFailed_System</code>アラームを設定すると、インスタンスを自動的に復旧してくれるEC2 Auto Recoveryという機能があり、使うためには条件がいくつかあります。</p>

<ul>
<li>特定のリージョン</li>
<li>C3, C4, M3, R3, T2 instances</li>
<li>VPC</li>
<li>共有tenancy</li>
<li>EBSストレージのみのサーバ</li>
</ul>


<p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html">Recover Your Instance</a></p>

<p>しかし、上記を満たしているのに、特定のAMIをCLI経由で起動するとEC2 Auto Recoveryを設定できなくなります。</p>

<p>（AWSコンソールでラジオボタンが押せない。CLIで設定しても効かない）
<img src="https://lh5.googleusercontent.com/-dwWUcfCssA4/VULwrOsgxNI/AAAAAAAABFo/VQy-GnIYQXg/w448-h186-no/Screenshot%2B2015-05-01%2B12.18.16.png"></p>

<p>原因はAMIにephemeral disk等のblock device mappingが設定されていて、T2やC4等のEBS onlyなinstance typeで起動しているにも関わらず、AWS側がephemeral diskが付与されていると認識してしまう所にあります。なお、AWSコンソールからの起動だとこの現象は発生しません。</p>

<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>


<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.1/gist-embed.min.js"></script>


<p>AMIでblock device mappingが埋め込まれている</p>

<pre><code>aws ec2 describe-images --image-ids ami-e74b60e6 --region ap-northeast-1
</code></pre>

<p><code data-gist-id="55659515d54593c29618" data-gist-highlight-line="21-28"></code></p>

<p>本来はEBS onlyなinstance typeだとblock device mappingの設定如何に関わらず、付与自体が不可能なので、全く関係ないはずです。</p>

<p>解決方法は現時点（2015/5/1）では3通り</p>

<ul>
<li>extraなblock device mappingが設定されていないAMIを使う（Amazon Linux等）</li>
<li>AWSコンソールから起動する</li>
<li>明示的に<code>--block-device-mappings</code>パラメータで<code>NoDevice</code>と指定
<code>aws ec2 run-instances --image-id ami-e74b60e6 --instance-type t2.small --subnet-id subnet-xxxxxxxx --block-device-mappings "[{\"DeviceName\": \"/dev/sdb\",  \"NoDevice\": \"\"}, {\"DeviceName\": \"/dev/sdc\",  \"NoDevice\": \"\"}]"</code></li>
</ul>


<p>これは明らかにAWS側のバグなので早く治って欲しいものですね。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Auto Scalingによる自動復旧（AWS Lambda+SNS編）]]></title>
    <link href="http://ijin.github.io/blog/2015/04/17/self-healing-with-non-elb-autoscaling4/"/>
    <updated>2015-04-17T14:01:00+09:00</updated>
    <id>http://ijin.github.io/blog/2015/04/17/self-healing-with-non-elb-autoscaling4</id>
    <content type="html"><![CDATA[<p>先週の<a href="http://aws.amazon.com/summits/san-francisco/">AWS Summit San Fransisco</a>にて、ついにLambdaがSNSに<a href="http://docs.aws.amazon.com/sns/latest/dg/sns-lambda.html">対応</a>しました。
様々なサービスが発表された中、個人的にはこれが一番のヒットです！というのも、この機能によってAWS間のサービスがより連携しやすくなり、新しいリアクティブなアーキテクチャをどんどん実現できそうだからです。</p>

<p>というわけで、少し試してみました。</p>

<p>お題は去年12月に試したAutoScaling + Lambda。（当時はLambdaはまだこの機能がなかったのでSNS→SQSにてイベントをプロセスする仕組みを<a href="http://ijin.github.io/blog/2014/12/05/self-healing-with-non-elb-autoscaling3/">作りました</a>。）</p>

<p>SNS連携によって前回のこれが</p>

<p><img src="https://lh4.googleusercontent.com/-IxSeVgkwfQU/VIIapiet4tI/AAAAAAAABBw/ukic0BIBIT0/w529-h393-no/aws-advent-2014.png"></p>

<p>こうなります。（Lambdaのアイコンが出たので差し替えてます）</p>

<p><img src="https://lh3.googleusercontent.com/-ejPyB1qrZyQ/VTCEjbhe2cI/AAAAAAAABFI/VCYVIo5hFao/w404-h393-no/as-sns-lambda2.png"></p>

<p>うーん、シンプル！</p>

<h3>設定</h3>

<p>前回とほぼ同様。</p>

<p>SNS作成</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ aws sns create-topic --name instance-alert --region ap-northeast-1
</span><span class='line'>{
</span><span class='line'>    "TopicArn": "arn:aws:sns:ap-northeast-1:123456789012:instance-alert"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>LambdaとSNS連携できるようにポリシーを付与</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ aws lambda add-permission --function-name makeASUnhealty --statement-id sns-instance-alert \
</span><span class='line'>--action "lambda:invokeFunction" --principal sns.amazonaws.com \
</span><span class='line'>--source-arn arn:aws:sns:ap-northeast-1:123456789012:instance-alert --region us-west-2                                                                                                                                                                                                  
</span><span class='line'>{
</span><span class='line'>    "Statement": "{\"Condition\":{\"ArnLike\":{\"AWS:SourceArn\":\"arn:aws:sns:ap-northeast-1:123456789012:instance-alert\"}},\"Resource\":\"arn:aws:lambda:us-west-2:123456789012:function:makeASUnhealty\",\"Action\":[\"lambda:invokeFunction\"],\"Principal\":{\"Service\":\"sns.amazonaws.com\"},\"Sid\":\"sns-instance-alert\",\"Effect\":\"Allow\"}"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Subscribe</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ aws sns subscribe --topic-arn arn:aws:sns:ap-northeast-1:123456789012:instance- --protocol lambda \
</span><span class='line'>--notification-endpoint arn:aws:lambda:us-west-2:123456789012:function:makeASUnhealty --region ap-northeast-1
</span><span class='line'>{
</span><span class='line'>    "SubscriptionArn": "arn:aws:sns:ap-northeast-1:123456789012:as-test:4b22eec6-aeb5-4421-7a2f-99ca33a4b8ab"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>aws cliはのヘルプにはまだSNSをLambdaへsubscribeするやり方は書いてませんが、上記のようにやればできます。 <a href="http://alestic.com/2015/04/aws-cli-sns-lambda">Thanks Eric!</a></p>

<p>EC2 Status Check</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ export INSTANCE=i-xxxxxxxx
</span><span class='line'>$ aws cloudwatch put-metric-alarm --alarm-name StatusCheckFailed-Alarm-for-$INSTANCE \
</span><span class='line'>--alarm-description "Instance $INSTANCE has failed" --metric-name StatusCheckFailed \
</span><span class='line'>--namespace AWS/EC2 --statistic Maximum --dimensions Name=InstanceId,Value=$INSTANCE \
</span><span class='line'>--period 60 --unit Count --evaluation-periods 2 --threshold 1 --comparison-operator \
</span><span class='line'>  GreaterThanOrEqualToThreshold --alarm-actions arn:aws:sns:ap-northeast-1:560336700862:instance-alert \
</span><span class='line'>--region ap-northeast-1</span></code></pre></td></tr></table></div></figure>


<p>Lambda Function</p>

<div><script src='https://gist.github.com/52033eb3b9b02c1fe975.js?file='></script>
<noscript><pre><code></code></pre></noscript></div>


<h3>自動復旧</h3>

<p>通信を遮断し、Status Check Failを発動させる</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ date; sudo ifdown eth0
</span><span class='line'>Fri Apr 17 03:08:39 UTC 2015</span></code></pre></td></tr></table></div></figure>


<p>EC2 Status Check。2分でfail検知</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Fri Apr 17 12:10:25 JST 2015
</span><span class='line'>ok
</span><span class='line'>DETAILS reachability    passed
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:10:31 JST 2015
</span><span class='line'>impaired
</span><span class='line'>DETAILS 2015-04-17T03:10:00.000Z        reachability    failed</span></code></pre></td></tr></table></div></figure>


<p>SNS通知。さらに2分ちょい。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Alarm Details:
</span><span class='line'>- Name:                       StatusCheckFailed-Alarm-for-i-xxxxxxxx
</span><span class='line'>- Description:                Instance i-xxxxxxxx has failed
</span><span class='line'>- State Change:               OK -&gt; ALARM
</span><span class='line'>- Reason for State Change:    Threshold Crossed: 2 datapoints were greater than or equal to the threshold (1.0). The most recent datapoints: [1.0, 1.0].
</span><span class='line'>- Timestamp:                  Friday 17 April, 2015 03:13:09 UTC
</span><span class='line'>- AWS Account:                123456789012
</span><span class='line'>
</span><span class='line'>Threshold:
</span><span class='line'>- The alarm is in the ALARM state when the metric is GreaterThanOrEqualToThreshold 1.0 for 60 seconds.
</span><span class='line'>
</span><span class='line'>Monitored Metric:
</span><span class='line'>- MetricNamespace:            AWS/EC2
</span><span class='line'>- MetricName:                 StatusCheckFailed
</span><span class='line'>- Dimensions:                 [InstanceId = i-xxxxxxxx]
</span><span class='line'>- Period:                     60 seconds
</span><span class='line'>- Statistic:                  Maximum
</span><span class='line'>- Unit:                       Count</span></code></pre></td></tr></table></div></figure>


<p>Lambdaログ</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2015-04-17T03:13:14.504Z  ac9ed52f-e4af-1e14-b826-ee9008a99db9   Received event:..
</span><span class='line'>2015-04-17T03:13:14.504Z  ace9d52f-e4af-1e14-b826-ee9008a99db9    Changing instance health for: i-xxxxxxxx
</span><span class='line'>2015-04-17T03:13:15.682Z  ace9d25f-e4af-1e14-b826-ee9008a99db9    { ResponseMetadata: { RequestId: 'b0194dfb-e4af-1e14-895f-abdf96b0b593' } }
</span><span class='line'>2015-04-17T03:13:15.684Z  ace9d25f-e4af-1e14-b826-ee9008a99db9    result: ""</span></code></pre></td></tr></table></div></figure>


<p>タイムスタンプによるとSNS発砲されてたからLambda発火まで5秒！</p>

<p>Auto ScalingのHealth Status</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Fri Apr 17 12:13:10 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a HEALTHY i-ce02563d      as-lc   InService
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:13:15 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a UNHEALTHY       i-ce02563d      as-lc   InService
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:13:20 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a UNHEALTHY       i-ce02563d      as-lc   InService
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:13:26 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a UNHEALTHY       i-ce02563d      as-lc   InService
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:13:31 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a UNHEALTHY       i-ce02563d      as-lc   InService
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:13:37 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a UNHEALTHY       i-ce02563d      as-lc   Terminating
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:13:43 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a UNHEALTHY       i-ce02563d      as-lc   Terminating
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:13:49 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a UNHEALTHY       i-ce02563d      as-lc   Terminating
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:13:54 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a UNHEALTHY       i-ce02563d      as-lc   Terminating
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:13:59 JST 2015
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:14:05 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a HEALTHY i-525601a1      as-lc   Pending
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:14:10 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a HEALTHY i-525601a1      as-lc   Pending
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:14:16 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a HEALTHY i-525601a1      as-lc   Pending
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:14:21 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a HEALTHY i-525601a1      as-lc   Pending
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:14:26 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a HEALTHY i-525601a1      as-lc   Pending
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:14:32 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a HEALTHY i-525601a1      as-lc   Pending
</span><span class='line'>
</span><span class='line'>Fri Apr 17 12:14:37 JST 2015
</span><span class='line'>as-sg   ap-northeast-1a HEALTHY i-525601a1      as-lc   InService</span></code></pre></td></tr></table></div></figure>


<p>ちゃんとTerminateされてリプレースされてますね。</p>

<p>AutoScalingの通知</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Service: AWS Auto Scaling
</span><span class='line'>Time: 2015-04-17T03:13:59.367Z
</span><span class='line'>RequestId: efa97137-fa15-4aa4-9c8c-5241961a2d0e
</span><span class='line'>Event: autoscaling:EC2_INSTANCE_TERMINATE
</span><span class='line'>AccountId: 123456789012
</span><span class='line'>AutoScalingGroupName: as-sg
</span><span class='line'>AutoScalingGroupARN: arn:aws:autoscaling:ap-northeast-1:123456789012:autoScalingGroup:c395c157-3a7e-4d56-287b-5ad9b26eb464:autoScalingGroupName/as-sg
</span><span class='line'>ActivityId: efa97137-fa15-4aa4-9c8c-5241961a2d0e
</span><span class='line'>Description: Terminating EC2 instance: i-xxxxxxxx
</span><span class='line'>Cause: At 2015-04-17T03:13:36Z an instance was taken out of service in response to a user health-check.
</span><span class='line'>StartTime: 2015-04-17T03:13:36.342Z
</span><span class='line'>EndTime: 2015-04-17T03:13:59.367Z
</span><span class='line'>StatusCode: InProgress
</span><span class='line'>StatusMessage:
</span><span class='line'>Progress: 50
</span><span class='line'>EC2InstanceId: i-xxxxxxxx
</span><span class='line'>Details: {"Availability Zone":"ap-northeast-1a","Subnet ID":"subnet-bbbbbbbb"}</span></code></pre></td></tr></table></div></figure>


<p>通常は<code>Cause</code>が<code>an instance was taken out of service in response to a EC2 health check indicating it has been terminated or stopped.</code>となるのが<code>an instance was taken out of service in response to a user health-check.</code>となっているのでAutoScalingのEC2 Health Checkより前にアクションが起こされた事が分かります。</p>

<p>障害発生からInstanceがリプレースされて<code>InService</code>になるAuto Healingのトータル時間は6分ちょいになりました。
<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html">EC2 Auto Recovery</a>を使えば済む場合もありますが、あちらはAWS側の障害に起因する<code>StatusCheckFailed_System</code>のみで<code>StatusCheckFailed_Instance</code>はトリガー対象じゃないのと、特定のインスタンスタイプやVPC等若干制限があります。</p>

<h3>終わりに</h3>

<p>ちなみに今回はinstanceやSNSは東京リージョン（ap-northeast-1）、Lambdaはオレゴンリージョン（us-west-2）というリージョンを跨いだ連携も可能という事が分かりました。まだ東京に来てないけど、既にproduction readyなのでもう普通に使っていけます。</p>

<p>いやー。SNS連携によって夢は広がりますねぇ。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[サバフェス！2015に参加してきた]]></title>
    <link href="http://ijin.github.io/blog/2015/03/27/serverfesta-2015-spring/"/>
    <updated>2015-03-27T12:56:00+09:00</updated>
    <id>http://ijin.github.io/blog/2015/03/27/serverfesta-2015-spring</id>
    <content type="html"><![CDATA[<p><a href="http://connpass.com/event/11571/">サバフェス！2015 Spring</a>に参加してきました。
やった内容を少々。</p>

<p><a href="http://ijin.github.io/blog/2013/12/13/serverfesta-2013-autumn">前回</a>と同じくチーム@ijinとして1人で参戦して、順位は「<strong>4位</strong>」。</p>

<p>スコアは<strong>44988.867 TpmC</strong>でした。</p>

<h2>お題</h2>

<p>Mysql on ioDriveでtpcc-mysqlベンチマークのtransaction throughput競争。</p>

<p>第1陣と第2陣に分かれていて、今回は後者での参戦。</p>

<h2>はじめに</h2>

<p>競技期間は5日間あったものの、第1陣で<a href="http://netmark.jp/2015/03/svfes-2.html">結構な地雷があった</a>のと、Fusion-IO（現SanDisk）のioDriveとtpcc-mysqlは2年前に<a href="http://ijin.github.io/blog/2013/02/22/mysql-benchmarks-on-aws-ssd-vs-fusion-io/">触った</a>ので最初はあんまりやる気が起きなくて困ってました。後は前回と違って施せる施策がかなり限定されるというので、正直5日間は長過ぎるのではないかという印象でした。（結果的に第1陣のトラブルとかを鑑みると長さ的には良かったのかも知れないけど）</p>

<p>という事で初日はログインとベンチで基準値を取るだけして終了。</p>

<p>その後、最近のMySQLやioDrive周りの情報収集を軽ーくして数日が経過。。</p>

<p>そして最終日の夜中になって、なんとか本腰を入れてチューニング開始。まあ、やりだしたら楽しいんですけどね。</p>

<h2>マシンスペック</h2>

<ul>
<li>CentOS 6.4</li>
<li>Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz x 32</li>
<li>32GB RAM</li>
<li>40G HDD</li>
<li>320GB ioDrive</li>
</ul>


<h2>制限</h2>

<pre><code>innodb_doublewrite = 1
innodb_flush_log_at_trx_commit = 1
</code></pre>

<p>その他レギューレーションは<a href="https://2015spring.serverfesta.info/?page_id=299">こちら</a></p>

<h2>方針</h2>

<p>今までのtwitterの<a href="https://twitter.com/search?q=%23%E3%82%B5%E3%83%90%E3%83%95%E3%82%A7%E3%82%B9">タイムライン</a>からして、どうもベンチをウェブ画面から要求しても並列実行の数とキューイング、及び非常に長い実行時間からして1時間に一回実行できるかどうかも怪しかったのでローカルでこまめに回す方向に。</p>

<p>本番データはtpccの1000 warehouse（70GB+）、16GB Buffer Pool、900sの実行時間だったのでそれを250 warehouse, 4GB Buffer Poolと縮小し120sと短めに設定。こうすることによってローカルでのデータコピー時間やベンチ実行時感が短縮され、時間がない中での細かいパラメータのチューニングに専念できる。Buffer Poolはどうせ一番効くと分かりきってるので、あえて後回し。DBは6年前（MySQL 5.0時代）から手に滲んで愛用しているPercona Serverの5.6版を選択。</p>

<p>スコアの推移はこんな感じ。データセットが小さい分、スコアは大きめ。</p>

<p><img src="https://docs.google.com/spreadsheets/d/1zm6-THRYR_EcLoHgacRScxlhSzeQAkbJ0RXiAI4fiRU/pubchart?oid=1067951178&amp;format=image"></p>

<p>終盤になってやっと本番に近いデータセットや実行時感で細かいチューニング。</p>

<p><img src="https://docs.google.com/spreadsheets/d/1zm6-THRYR_EcLoHgacRScxlhSzeQAkbJ0RXiAI4fiRU/pubchart?oid=2026636320&amp;format=image"></p>

<p>最終的にローカルでのベストスコアは<strong>53420.332 TpmC</strong>でした。</p>

<h2>設定ファイル</h2>

<script src="https://gist.github.com/ijin/341bab7569e372e1addb.js"></script>


<h2>効果があったもの</h2>

<p>細かいおまじないレベルでのパラメータも他にいくつあったけど、割と効いたのをピックアップ。また、既に設定されていたパラメータは除外（innodb_io_capacity等）</p>

<p>mysql</p>

<pre><code>datadir=/fioa/mysql
tmpdir=/fioa/tmp
sync_binlog=0
innodb_buffer_pool_size = 28150M
innodb_buffer_pool_instances=16
innodb_log_file_size=4G
innodb_log_files_in_group=3
innodb_log_group_home_dir=/var/log/mysql
innodb_log_buffer_size=64M
innodb_data_file_path=ibdata1:76M;../../var/log/mysql/ibdata2:500M:autoextend
innodb_checksum_algorithm=0
innodb_max_dirty_pages_pct=90
innodb_lru_scan_depth=2000
numa_interleave=1
flush_caches
malloc-lib=/usr/lib64/libjemalloc.so.1
</code></pre>

<p>etc</p>

<pre><code>vm.swappiness=1
mount option (noatime,nodiratime,  max_batch_time=0,nobarrier,discard)
cache warmup
</code></pre>

<p>基本的にはioDriveにIOをなるべく発生させない、もしくは遅延させる関連のパラメータが効いた感じ。この辺は割と正統なチューニング方法。一つ、若干工夫したのは、doublewrite buffer fileの指定方法。シーケンシャルなIOが発生するログ周りの処理はHDDに逃がした方がioDrive/SSD的には負荷低減になるけど、Percona 5.5までは<strong>innodb_doublewrite_file</strong>というパラメータでいつも指定していたのが、5.6ではなんと未実装！なのでベンチマーク前にコピーされるibdata1の予めのサイズを計っておいて、以降の書き込みをHDD側へ逃がすように<strong>innodb_data_file_path</strong>で調整。</p>

<p>他はNUMAによるメモリの偏り調整とmallocライブラリを使う指定。この辺もPerconaやMariaDB専用オプション。</p>

<p>後は一応初動のスコアをちょっとだけ稼ぐためにmysqlのstartup script内にてテーブルをcount(*)してindexをbuffer poolに乗せたぐらい。</p>

<h2>効果が微妙だったもの</h2>

<p>mysql</p>

<pre><code>innodb_flush_method=ALL_O_DIRECT
innodb_support_xa=0
innodb_thread_concurrency=N
innodb_flush_neighbors=0
query_cache_size=0
large-pages
</code></pre>

<p>etc</p>

<pre><code>echo 'noop' &gt; /sys/block/fioa/queue/scheduler
echo 4096 &gt; /sys/block/fioa/queue/nr_requests
echo 2 &gt; /sys/block/fioa/queue/rq_affinity
renice -n19 -p `ps auxf | grep mackerel | grep -v grep | awk ‘{print $2}’`
</code></pre>

<p>large-pagesでページサイズを大きく設定すればメモリ効率は向上するはずが、少なくともベンチマークにおいては効果なし。また、スケジューラをnoopやdeadlineと変えたり、nr_requestsやrq_affinityを調整してみたけど、デフォルトのOS bypass設定と比べてあまり変化なし。</p>

<h2>試したかったもの</h2>

<ul>
<li>ioDrvieのblock sizeのリサイズ</li>
<li>C0 state（CPUのC stateを制御して変動させずにioDriveの処理向上の期待）</li>
<li>numa_node_forced_local（IO処理をメモリに一番近接しているCPUで行う）</li>
<li>IRQ pinning（ioDriveのIRQを固定）</li>
<li>network tuning</li>
</ul>


<p>この中で最後やることリストに乗っけていながらやらなかったネットワーク周りのチューニング。多分、これが敗因。（1位のチームzzz(<a href="https://twitter.com/ttkzw">@ttkzwさん</a>)はローカルベンチマークで51000ぐらいだったので）もっとリモートからベンチが実行される事に意識を向ければ良かったのかもしれないですね。とはいえ、最後はキュー待ちが8チームだったり、結果が不具合で見れなかったりと、バダバタしてたのでそっちに気を取られたのもまた事実。まあ、半日ちょいのチューニングにしてはそこそこ行った感じでしょうか。</p>

<h2>終わりに</h2>

<p>最初は運営側が「目下実装中です！」といいながら<strong>#トラしゅ</strong>をしているような感じでちょっと不安でしたが、最後は（第2陣という事もあり）それなりに楽しめました。ベンチマーク時間の短縮と並列実行数がもうちょっと増やせたらよかったですかね。運営側の皆様、大変お疲れさまでした！</p>

<h2>おまけ</h2>

<p>賞品として3DマッサージピローとSSDを頂きました！</p>

<div class='embed tweet'><blockquote class="twitter-tweet"><p>3DマッサージピローとSSDをもらった。 <a href="https://twitter.com/hashtag/%E3%82%B5%E3%83%90%E3%83%95%E3%82%A7%E3%82%B9?src=hash">#サバフェス</a> <a href="http://t.co/mKczn9mflZ">pic.twitter.com/mKczn9mflZ</a></p>&mdash; Michael H. Oshita (@ijin) <a href="https://twitter.com/ijin/status/581048257171820544">March 26, 2015</a></blockquote>
<script async src="http://ijin.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script></div>


<p>個人敵には飛び込みLTした人がもらったDroneの方が良さそうだったけど、ピローは家族に好評でした。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ISUCON4の本戦の思い出]]></title>
    <link href="http://ijin.github.io/blog/2014/12/31/isucon4-final/"/>
    <updated>2014-12-31T20:52:00+09:00</updated>
    <id>http://ijin.github.io/blog/2014/12/31/isucon4-final</id>
    <content type="html"><![CDATA[<p>ISUCON4の本戦の直後にそのままAWS re:Inventへ行ったきりエントリを書いてなかったので思い出だけでも年内に書いておく。</p>

<p>内容の<a href="http://isucon.net/archives/41252218.html">エントリ</a>はたくさんあるので、詳しい内容なそちらに任せます。</p>

<p>結果は予選よりちょい上の<strong>15位</strong>。まあ、中間層は結構団子状態だったので誤差の範囲とも言えますが。。</p>

<h2>Cache-Control</h2>

<p>結局はCache-Controlヘッダーに気づくかどうかという点にかかっていて、達成したのは<a href="http://isucon.net/archives/41634734.html">2チーム</a>のみ。優勝チームのモリスさんが「頭から煙が出る程考えて」やっと直前に答えにたどり着いた事を考えると、ベンチマークツールの挙動が若干不思議な実装になっていたとはいえ、思慮深さが足りなかったと反省。</p>

<h2>冒険しすぎた</h2>

<p>今回はrubyとGoのハイブリッド構成でもろもろチューニング。が、しばらくして帯域が頭打ちに。。
また、その間にメンバーの一人の<a href="https://twitter.com/fruwe">@fruwe</a>に<a href="http://undertow.io/">Undertow</a>というjavaのフレームワークでチャレンジしてもらうものの（事前の技術検証では結構期待できそうだった）、実装完走にはいたらず。この辺は冒険し過ぎたかな。。振り返ってみれば、結局2位のチームがブレークスルーをしたのを見て、全作業を一旦ストップしてでも皆で考えなおす行動を取れば良かったのかもしれないですね。ただ、当時は予選でのベンチマークツールがいささか不安定だった事と運営側もリアルタイムでバグフィックスをしてたという事情もあり、きっとバグか何かだろうとあまり気にしてなかったのも確か。</p>

<h2>最後に</h2>

<p>とまあ、不完全燃焼だったけど、なんだかんだで面白かったです。運営＆参加者の皆様お疲れ様でした。来年またあるか分からないけど、楽しみにしています。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Auto Scalingによる自動復旧（AWS Lambda編）]]></title>
    <link href="http://ijin.github.io/blog/2014/12/05/self-healing-with-non-elb-autoscaling3/"/>
    <updated>2014-12-05T23:14:00+09:00</updated>
    <id>http://ijin.github.io/blog/2014/12/05/self-healing-with-non-elb-autoscaling3</id>
    <content type="html"><![CDATA[<p>ちょうど1年程前に「<a href="http://ijin.github.io/blog/2013/02/08/self-healing-with-non-elb-autoscaling/">非ELBなAutoscalingによる自動復旧</a>」の<a href="http://ijin.github.io/blog/2013/12/14/self-healing-with-non-elb-autoscaling2/">再検証</a>をしました。前回も復旧までのタイムラグが20分だったので、この1年で変わったかまた検証してみました。</p>

<p>(*) このエントリは<a href="http://qiita.com/advent-calendar/2014/aws">AWS Advent Calendar 2014</a>の5日目分です。</p>

<h3>設定</h3>

<p><a href="http://ijin.github.io/blog/2013/12/14/self-healing-with-non-elb-autoscaling2/">前回</a>とほぼ一種ですが、今回はついでにEC2 Status AlarmをCloudwatch経由でSNSでアラートを飛ばします。</p>

<p>SNS作成 &amp; Subscribe（送られてくる確認メールは手動で承認）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ aws sns create-topic --name instance-alert
</span><span class='line'>{
</span><span class='line'>    "TopicArn": "arn:aws:sns:us-west-2:123456789012:instance-alert"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>$ aws sns subscribe --topic-arn arn:aws:sns:us-west-2:123456789012:instance-alert --protocol email --notification-endpoint user@example.com                               
</span><span class='line'>{
</span><span class='line'>    "SubscriptionArn": "pending confirmation"
</span><span class='line'>}
</span></code></pre></td></tr></table></div></figure>


<p>EC2 Status Alarm登録</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export INSTANCE=i-xxxxxxxx
</span><span class='line'>aws cloudwatch put-metric-alarm --alarm-name StatusCheckFailed-Alarm-for-$INSTANCE \
</span><span class='line'>--alarm-description "Instance $INSTANCE has failed" --metric-name StatusCheckFailed \
</span><span class='line'>--namespace AWS/EC2 --statistic Maximum --dimensions Name=InstanceId,Value=$INSTANCE \
</span><span class='line'>--period 60 --unit Count --evaluation-periods 2 --threshold 1 --comparison-operator \
</span><span class='line'>  GreaterThanOrEqualToThreshold --alarm-actions arn:aws:sns:us-west-2:123456789012:instance-alert</span></code></pre></td></tr></table></div></figure>


<h3>自動復旧</h3>

<p>通信を遮断し、Status Check Failを発動させる</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ubuntu@ip-172-31-19-185:~$ date; sudo ifdown eth0
</span><span class='line'>Fri Dec  5 13:02:39 UTC 2014</span></code></pre></td></tr></table></div></figure>


<p>EC2のStatus Check。前回同様、3分ぐらいでfail検知</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Fri Dec  5 22:05:22 JST 2014
</span><span class='line'>ok
</span><span class='line'>DETAILS reachability    passed
</span><span class='line'>
</span><span class='line'>Fri Dec  5 22:05:28 JST 2014
</span><span class='line'>ok
</span><span class='line'>DETAILS reachability    passed
</span><span class='line'>
</span><span class='line'>Fri Dec  5 22:05:34 JST 2014
</span><span class='line'>impaired
</span><span class='line'>DETAILS 2014-12-05T13:05:00.000Z        reachability    failed
</span><span class='line'>
</span><span class='line'>Fri Dec  5 22:05:40 JST 2014
</span><span class='line'>impaired
</span><span class='line'>DETAILS 2014-12-05T13:05:00.000Z        reachability    failed</span></code></pre></td></tr></table></div></figure>


<p>SNS通知。飛ぶまで2分弱</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Alarm Details:
</span><span class='line'>- Name:                       StatusCheckFailed-Alarm-for-i-xxxxxxxx
</span><span class='line'>- Description:                Instance i-xxxxxxxx has failed
</span><span class='line'>- State Change:               OK -&gt; ALARM
</span><span class='line'>- Reason for State Change:    Threshold Crossed: 2 datapoints were greater than or equal to the threshold (1.0). The most recent datapoints: [1.0, 1.0].
</span><span class='line'>- Timestamp:                  Friday 05 December, 2014 13:07:19 UTC
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>- AWS Account:                123456789012
</span><span class='line'>
</span><span class='line'>Threshold:
</span><span class='line'>- The alarm is in the ALARM state when the metric is GreaterThanOrEqualToThreshold 1.0 for 60 seconds.
</span><span class='line'>
</span><span class='line'>Monitored Metric:
</span><span class='line'>- MetricNamespace:            AWS/EC2
</span><span class='line'>- MetricName:                 StatusCheckFailed
</span><span class='line'>- Dimensions:                 [InstanceId = i-xxxxxxxx]
</span><span class='line'>- Period:                     60 seconds
</span><span class='line'>- Statistic:                  Maximum
</span><span class='line'>- Unit:                       Count</span></code></pre></td></tr></table></div></figure>


<p>AutoscalingのHealth Status</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Fri Dec  5 22:11:08 JST 2014
</span><span class='line'>aws-advent2014-sg       us-west-2a      HEALTHY i-xxxxxxxx      aws-advent2014-lc       InService
</span><span class='line'>
</span><span class='line'>Fri Dec  5 22:11:14 JST 2014
</span><span class='line'>aws-advent2014-sg       us-west-2a      HEALTHY i-xxxxxxxx      aws-advent2014-lc       InService
</span><span class='line'>
</span><span class='line'>Fri Dec  5 22:11:20 JST 2014
</span><span class='line'>aws-advent2014-sg       us-west-2a      UNHEALTHY       i-xxxxxxxx      aws-advent2014-lc       InService
</span><span class='line'>
</span><span class='line'>Fri Dec  5 22:11:26 JST 2014
</span><span class='line'>aws-advent2014-sg       us-west-2a      UNHEALTHY       i-xxxxxxxx      aws-advent2014-lc       InService
</span><span class='line'>
</span><span class='line'>Fri Dec  5 22:11:32 JST 2014
</span><span class='line'>aws-advent2014-sg       us-west-2a      UNHEALTHY       i-xxxxxxxx      aws-advent2014-lc       Terminating
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>お、4分ぐらいでAuto ScalingがUnhealthyと認識。</p>

<p>何回か繰り返したところ、トータルで障害から復旧までの時間が10分を切りました！</p>

<p>なんと1年前と比べて時間が半分に短縮されてますねぇ。</p>

<h2>Lambda</h2>

<p>さて。改善されたものの、Auto ScalingがEC2 status checkの状態を検知するまでタイムラグがまだあるので、もうちょっと短縮したいですよね。できればSNSが発行されたタイミングで。</p>

<p>そこで、AWSの新サービス「<a href="http://aws.amazon.com/lambda/">Lambda</a>」を使ってイベント通知できたら良いかも！！。。と思ったものの、残念ながらLambdaはまだSNSには対応してません。</p>

<p>なので、ひとまずSQSをSNSにsubscribeして、messageが届いたらLambda functionへ渡してinvokeへしてくれるsqs-to-lambdaを使ってAuto ScalingのHealthStatusを直接API経由でLambdaが叩く仕組みを試しました。
図にするとこんな感じですね。</p>

<p><img src="https://lh4.googleusercontent.com/-IxSeVgkwfQU/VIIapiet4tI/AAAAAAAABBw/ukic0BIBIT0/w529-h393-no/aws-advent-2014.png"></p>

<p>ELB付けた方が楽な気もするけど、まあ集約できるのと検証も兼ねて。。</p>

<h3>設定</h3>

<p>SQSの作成</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ aws sqs create-queue --queue-name instance-failed
</span><span class='line'>{
</span><span class='line'>    "QueueUrl": "https://us-west-2.queue.amazonaws.com/123456789012/instance-failed"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>SQSをSNSへsubscribe</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ aws sqs add-permission --queue-url https://us-west-2.queue.amazonaws.com/123456789012/instance-failed \
</span><span class='line'> --label SQSDefaultPolicy --aws-account-ids  --actions SendMessage
</span><span class='line'>$ aws sqs set-queue-attributes --queue-url https://us-west-2.queue.amazonaws.com/123456789012/instance-failed  \
</span><span class='line'> --attributes '{"Policy": "{\"Version\":\"2008-10-17\",\"Id\":\"arn:aws:sqs:us-west-2:123456789012:instance-failed/SQSDefaultPolicy\",\"Statement\":[{\"Sid\":\"Sid1417796380309\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"*\"},\"Action\":\"SQS:SendMessage\",\"Resource\":\"arn:aws:sqs:us-west-2:123456789012:instance-failed\",\"Condition\":{\"ArnEquals\":{\"aws:SourceArn\":\"arn:aws:sns:us-west-2:123456789012:instance-alert\"}}}]}"}'</span></code></pre></td></tr></table></div></figure>


<p>Lambda function</p>

<div><script src='https://gist.github.com/7bcba3354814d3ca704d.js?file='></script>
<noscript><pre><code>console.log('Loading event');
var aws = require('aws-sdk');
var s3 = new aws.S3({apiVersion: '2006-03-01'});
var autoscaling = new aws.AutoScaling({apiVersion: '2011-01-01'});

exports.handler = function(event, context) {
   console.log('Received event:');
   console.log(event);
   //console.log(typeof event.Subject);
   if (event.hasOwnProperty('Message')) {
       var msg = event.Message.replace(/!!/g,'&quot;');
       var instance_id = JSON.parse(msg).Trigger.Dimensions[0].value;
       console.log('Changing instance health for: ' + instance_id);
       var params = {
           HealthStatus: 'Unhealthy', /* required */
           InstanceId: instance_id, /* required */
           ShouldRespectGracePeriod: false
       };
    
        autoscaling.setInstanceHealth(params, function(err, data) {
            if (err) {
                console.log(err, err.stack); // an error occurred
                context.done('error','error: '+err);
            }
            else {
                console.log(data);           // successful response
                context.done(null,'');
            }
        });
   }
   else {
       console.log('No message');
       context.done(null,'');
   }
};</code></pre></noscript></div>


<p>node.js製のsqs-to-lambda。long pollingしつつ、messageを取得後にLambda functionをinvokeしてくれる。12/5現在ではEscape characterがAWS/JDKやCLIから送信できないという<a href="https://forums.aws.amazon.com/thread.jspa?threadID=166893&amp;tstart=0">大きな問題</a>がある為、<a href="https://github.com/robinjmurphy/sqs-to-lambda">upstream</a>を少し<a href="https://github.com/ijin/sqs-to-lambda/commit/080f1dcbf5f8bb3f7f4a6e0abdde72dce7ce5553">改修</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo apt-get install nodejs npm
</span><span class='line'>sudo ln -s /usr/bin/nodejs /usr/local/bin/node
</span><span class='line'>git@github.com:ijin/sqs-to-lambda.git
</span><span class='line'>cd sqs-to-lambda
</span><span class='line'>npm install
</span><span class='line'>./index.js --queueUrl https://sqs.us-west-2.amazonaws.com/123456789012/instance-failed --functionName myFunction --region us-west-2  </span></code></pre></td></tr></table></div></figure>


<h3>Lambdaによる復旧</h3>

<p>通信を遮断し、Status Check Failを発動させる</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ubuntu@ip-172-31-21-180:~$ date; sudo ifdown eth0
</span><span class='line'>Fri Dec  5 19:37:32 UTC 2014</span></code></pre></td></tr></table></div></figure>


<p>EC2のStatus Check。約3分ぐらいでfail検知</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Sat Dec  6 04:40:09 JST 2014
</span><span class='line'>ok
</span><span class='line'>DETAILS reachability    passed
</span><span class='line'>
</span><span class='line'>Sat Dec  6 04:40:16 JST 2014
</span><span class='line'>impaired
</span><span class='line'>DETAILS 2014-12-05T19:40:00.000Z        reachability    failed
</span><span class='line'>
</span><span class='line'>Sat Dec  6 04:40:22 JST 2014
</span><span class='line'>impaired
</span><span class='line'>DETAILS 2014-12-05T19:40:00.000Z        reachability    failed</span></code></pre></td></tr></table></div></figure>


<p>SNS通知。飛ぶまで2分弱</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Alarm Details:
</span><span class='line'>- Name:                       StatusCheckFailed-Alarm-for-i-yyyyyyyy
</span><span class='line'>- Description:                Instance i-yyyyyyyy has failed
</span><span class='line'>- State Change:               OK -&gt; ALARM
</span><span class='line'>- Reason for State Change:    Threshold Crossed: 2 datapoints were greater than or equal to the threshold (1.0). The most recent datapoints: [1.0, 1.0].
</span><span class='line'>- Timestamp:                  Friday 05 December, 2014 19:42:33 UTC
</span><span class='line'>- AWS Account:                123456789012 </span></code></pre></td></tr></table></div></figure>


<p>そしてLambda発動！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>START RequestId: e64849be-7cb6-12e4-9951-11b87edac46e
</span><span class='line'>2014-12-05T19:42:36.728Z        e64849be-7cb6-12e4-9951-11b87edac46e    Received event:
</span><span class='line'>2014-12-05T19:42:36.728Z        e64849be-7cb6-12e4-9951-11b87edac46e    { Type: 'Notification', MessageId: '2d85a5d8-c596-5f83-94a9-e924c97fd676', TopicArn: 'arn:aws:sns:us-west-2:123456789012:instance-alert', Subject: 'Status Check Alarm: !!StatusCheckFailed-Alarm-for-i-yyyyyyyy!! in US-West-2', Message: '{!!AlarmName!!:!!StatusCheckFailed-Alarm-for-i-yyyyyyyy!!,!!AlarmDescription!!:!!Instance i-yyyyyyyy has failed!!,!!AWSAccountId!!:!!123456789012!!,!!NewStateValue!!:!!ALARM!!,!!NewStateReason!!:!!Threshold Crossed: 2 datapoints were greater than or equal to the threshold (1.0). The most recent datapoints: [1.0, 1.0].!!,!!StateChangeTime!!:!!2014-12-05T19:42:33.757+0000!!,!!Region!!:!!US-West-2!!,!!OldStateValue!!:!!OK!!,!!Trigger!!:{!!MetricName!!:!!StatusCheckFailed!!,!!Namespace!!:!!AWS/EC2!!,!!Statistic!!:!!MAXIMUM!!,!!Unit!!:!!Count!!,!!Dimensions!!:[{!!name!!:!!InstanceId!!,!!value!!:!!i-yyyyyyyy!!}],!!Period!!:60,!!EvaluationPeriods!!:2,!!ComparisonOperator!!:!!GreaterThanOrEqualToThreshold!!,!!Threshold!!:1.0}}', Timestamp: '2014-12-05T19:42:33.841Z', SignatureVersion: '1', Signature: '1IYhSVfZmNxWWzoc539jBDN2HCo0Y5k/dUhWbaAEZSt/tkISjFkNTb9VsVwNAfZDOaLneO/sE2PwfUc/3aU9eedlAassxHOXAB6h844NVKxJzR5Xwg4dUx0mIb+fk9pMy/elcwk13GbDxLJ1cCTef7Bu7zyJU3TAF626YfAVhI9QdEo4o44g/y2osEXb+CuvFc5ICYpIWAad7gM5YPYxCU6tJ/CEtWGzaPz+O5Vk4NLm2/AizZ6LKA8/zqhQkqwnUwhzQDwuDGbJ2DXtTJwAO2r4M+zU8RwOxwPgEdgxA270xrmB6AlWV0mhsQIqqJVxo5Xm2v7y3iNUjKfov5DCZm==', SigningCertURL: 'https://sns.us-west-2.amazonaws.com/SimpleNotificationService-ad6697a11189d5c6f9eccf214ff9e123.pem', UnsubscribeURL: 'https://sns.us-west-2.amazonaws.com/?Action=Unsubscribe&SubscriptionArn=arn:aws:sns:us-west-2:123456789012:instance-alert:2d85a5d8-faea-5f83-baa1-9fecd7a5e71b' }
</span><span class='line'>2014-12-05T19:42:36.728Z        e64849be-7cb6-12e4-9951-11b87edac46e    Changing instance health for: i-yyyyyyyy
</span><span class='line'>014-12-05T19:42:36.831Z e64849be-7cb6-12e4-9951-11b87edac46e    { ResponseMetadata: { RequestId: 'dd605f39-7cb6-12e4-a2c2-d57010899d82' } }
</span><span class='line'>END RequestId: e64849be-7cb6-12e4-9951-11b87edac46e
</span><span class='line'>REPORT RequestId: e64849be-7cb6-12e4-9951-11b87edac46e  Duration: 175.86 ms     Billed Duration: 200 ms Memory Size: 128 MB     Max Memory Used: 18 MB</span></code></pre></td></tr></table></div></figure>


<p>AutoscalingのHealth Status</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Sat Dec  6 04:42:18 JST 2014
</span><span class='line'>aws-advent2014-sg       us-west-2a      HEALTHY i-yyyyyyyy      aws-advent2014-lc       InService
</span><span class='line'>
</span><span class='line'>Sat Dec  6 04:42:24 JST 2014
</span><span class='line'>aws-advent2014-sg       us-west-2a      HEALTHY i-yyyyyyyy      aws-advent2014-lc       InService
</span><span class='line'>
</span><span class='line'>Sat Dec  6 04:42:30 JST 2014
</span><span class='line'>aws-advent2014-sg       us-west-2a      UNHEALTHY       i-yyyyyyyy      aws-advent2014-lc       InService
</span><span class='line'>
</span><span class='line'>Sat Dec  6 04:42:35 JST 2014
</span><span class='line'>aws-advent2014-sg       us-west-2a      UNHEALTHY       i-yyyyyyyy      aws-advent2014-lc       InService
</span><span class='line'>
</span><span class='line'>Sat Dec  6 04:42:42 JST 2014
</span><span class='line'>aws-advent2014-sg       us-west-2a      UNHEALTHY       i-yyyyyyyy      aws-advent2014-lc       InService
</span><span class='line'>
</span><span class='line'>jSat Dec  6 04:42:48 JST 2014
</span><span class='line'>aws-advent2014-sg       us-west-2a      UNHEALTHY       i-yyyyyyyy      aws-advent2014-lc       Terminating</span></code></pre></td></tr></table></div></figure>


<p>おお。さらに短く5分ぐらいに短縮！！</p>

<h3>終わりに</h3>

<p>Lambdaと組み合わせる事によってSNS通知によるELBを使わないレスポンシブなAuto Scalingの自動復旧が実現できました。</p>

<p>折角のサーバいらずのLambdaの良さが全く活かされてないけど。。早くSNSにも対応して欲しいものです。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ISUCON4の予選を通過したんだった]]></title>
    <link href="http://ijin.github.io/blog/2014/10/20/isucon4-qualifier/"/>
    <updated>2014-10-20T15:38:00+09:00</updated>
    <id>http://ijin.github.io/blog/2014/10/20/isucon4-qualifier</id>
    <content type="html"><![CDATA[<p>そういえば、3回目の参加となる<a href="http://isucon.net/">ISUCON</a>の第4回目の予選を通過してました。</p>

<p><a href="http://isucon.net/archives/40576269.html">結果</a>は185チーム中、<strong>19位</strong>というなんと微妙な結果で本戦にいける事に。</p>

<h3>事前準備</h3>

<p>今年も<a href="https://twitter.com/cads">@cads</a>と<a href="https://twitter.com/fruwe">@fruwe</a>でお馴染みのメンバーで「Mr. Frank &amp; Co: A New Hope」として登録。</p>

<p>前日までこんな感じの打ち合わせや予習を実地。</p>

<ul>
<li>選択言語はruby</li>
<li>効率化・自動化の為にansible playbookを用意

<ul>
<li>ssh keys</li>
<li>dotfiles</li>
<li>rubyの複数バージョン</li>
<li>各種ミドルウェア</li>
</ul>
</li>
<li>sidekiq等のバックグラウンドワーカーやキャッシュ周りの仮組み</li>
<li>redisの復習</li>
<li>去年やったエントリを読む</li>
<li>当日の流れの確認</li>
<li>コミュニケーションツールとして最近仕事で活躍してる<strong>Slack</strong>を導入</li>
</ul>


<p>役割としては他の二人がコード実装に集中できるように、私がインフラ周り、コードレビュー、ファシリテーターを務める。</p>

<h3>お題発表</h3>

<p>不正ログイン防止の為にアクセスの失敗回数に応じてログインロックをかけるシステム。実に今風なオンラインバンキングサービス「いすこん銀行」w 最後はjsonのレポートが出力。</p>

<p><img src="https://lh5.googleusercontent.com/-XjuyJ-r62YE/VETj3ySpGkI/AAAAAAAABAo/2Q2yi6YWJUk/w622-h373-no/isucon4q-bank.png"></p>

<h3>前半戦</h3>

<p>まずは今までの経験からいきなりチューニングをせず、じっくりとアプリ挙動を把握し、計測し、コード解析に専念。レギュレーションもしっかり読む。この辺の衝動のコントロールはうまくなって来た感触。</p>

<p>アプリ自体は例年の作りによく似ていて、まあそこそこ高速化はできるだろうけど、その分他のチームも同様だろうと予想。</p>

<p>コードを3人で読みつつ、前日までに用意していたansibleを流したり、git化したり、計測を淡々と実地。</p>

<p>言語をいくつか参考値として計測した初期スコア</p>

<table>
<thead>
<tr>
<th>language </th>
<th> score </th>
</tr>
</thead>
<tbody>
<tr>
<td>ruby     </td>
<td> 1236 </td>
</tr>
<tr>
<td>go       </td>
<td> 1733 </td>
</tr>
<tr>
<td>perl     </td>
<td> 1662 </td>
</tr>
</tbody>
</table>


<ul>
<li>Web側は各リスエストの比率やレスポンスタイム等を解析</li>
<li>DB周りは幸いMySQLだったので全ログ出力してpt-query-digestで解析<br />
（Postgresだったらどうしたんだろう）</li>
<li>OS周りはdstatやhtop等でリソースの利用具合を観測</li>
</ul>


<p>計測やコードの理解が出来た時点で新アーキテクチャの設計を3人でディスカッションし、戦略立案。</p>

<p>タイムリミットをいくつか設け、いざ開始！（この時点ではまだ1行も変更してない）</p>

<p>基本戦略としてはMySQLのRedisへの置き換えし、アプリの処理自体を減らしていく方向。チームメイトの2人には実装を担当してもらい、その間に私が既存のMySQLバージョンのチューニングを施し、地味にスコアを上げていく。</p>

<ul>
<li>INDEX追加</li>
<li>MySQL parameter tuning</li>
<li>stylesheetsやjavascriptの直接配信</li>
<li>TCP tuning</li>
<li>File Descriptor上限緩和</li>
<li>unix domain socketの利用</li>
</ul>


<p>合間合間に声をかけ、実装具合や残り時間をチェックし、テンポをとる。</p>

<p>予め決めていた期限である14:00（ソフトリミット）に到達した時点で進捗を確認し、15:00（ハードリミット）までには間に合いそうだったのでそのまま続行を決定。</p>

<p>14:31にredis版が仕上がる！</p>

<p><strong>実装方法</strong></p>

<ul>
<li>初期化スクリプトでMySQLからRedisへ変換</li>
<li>失敗ログインはINCRでカウント</li>
<li>lockとban時にはSADDでSETに追加</li>
<li>SISMEMBERでlock/banの確認</li>
<li>ユーザ個別の履歴はHMSET/HMGETでハッシュ化</li>
<li>/reportはSMEMBERSで出力</li>
</ul>


<h3>後半戦</h3>

<p>Redis版のgit branchに私のコミットをマージし計測。MySQL版よりは多少良いスコア。</p>

<p>mysql version</p>

<pre><code>tag:benchmarker type:score      success:82910   fail:0  score:17910
</code></pre>

<p>redis version</p>

<pre><code>tag:benchmarker type:score      success:87810   fail:318  score:18969
</code></pre>

<p>CPU消費がアプリに移っていたので、後はいかにrubyに処理をさせないかの勝負となる。</p>

<p>ここからやったのは</p>

<ul>
<li>worker loadの最適値模索</li>
<li>パスワードを初期化スクリプト時にオンメモリで持つように改造</li>
<li>パスワードのハッシュ計算除外（ハッシュ計算のスキップは結果的にスコアには影響せず）</li>
<li>トップページのエラーリダイレクトをクエリパラメータ化し、nginxで静的キャッシュ</li>
<li>ログイン部分のlua実装（間に合わず）</li>
</ul>


<p>最後のlua実装は取り掛かったものの、期限までには時間が足りないと判断し断念。かわりに失格とならないように再起動後の正常動作する事を入念に確認。</p>

<p>結局、ハイスコアは<strong>39243</strong>で終了</p>

<pre><code>tag:benchmarker    type:score    success:181670    fail:0    score:39243
</code></pre>

<p>後で気づいたのが最高スコアではなく、最終提出スコアが最終結果となるので誤差により若干下がってしまった。。</p>

<p><strong>戦いの軌跡</strong></p>

<p><img src="https://lh6.googleusercontent.com/VDc46K_BClolTXevFw5IH4W9sS5Z6akC6FIpIAcJhTM=w925-h328-no"></p>

<h3>振り返り</h3>

<ul>
<li>戦略と遂行は概ね正しかったかと</li>
<li>タイムキープ大事</li>
<li>実装は段階的に作ったので大きなバグがなかった</li>
<li>ansible便利（自動化で効率化）</li>
<li>Slackが非常に良かった（特にgithub連携）</li>
<li>VarnishのTシャツ着てたのに使わなかった</li>
<li>access_logを切るのを忘れてた！（offで試した41594だった）</li>
<li>多くのチームが失格となっていたので最終チェックは大事</li>
<li>今回は基本実装が余裕で間に合ったので確実に上達していると実感</li>
</ul>


<h3>復習</h3>

<p>予選終了後、最後に断念したlua化を一人で実装してみた。</p>

<p>/login実装でスコアは<strong>5万超</strong>、/mypageまでやって<strong>6万超</strong>でした。</p>

<pre><code>tag:benchmarker type:report     count:banned ips        value:1043
tag:benchmarker type:report     count:locked users      value:5290
tag:worker      type:fail       reason:Response code should be 201, got 403     method:POST     uri:/results
tag:benchmarker type:fail       message: Score sending failed   reason:Response code should be 201, got 403     method:POST     uri:/results
tag:benchmarker type:score      success:278260  fail:0  score:60108
</code></pre>

<p>その後に、競技中には思いつなかったimageやcssをUser Agent判定で切って（DOM構造は変えずに）みたら<strong>15万超え</strong></p>

<pre><code>tag:benchmarker type:report     count:banned ips        value:3115
tag:benchmarker type:report     count:locked users      value:10426
tag:worker      type:fail       reason:Response code should be 201, got 403     method:POST     uri:/results
tag:benchmarker type:fail       message: Score sending failed   reason:Response code should be 201, got 403     method:POST     uri:/results
tag:benchmarker type:score      success:210324  fail:0  score:155272
</code></pre>

<p>この辺が限界。</p>

<p>うーむ。山形組の<a href="http://nihen.hatenablog.com/entry/2014/10/01/092938">30万超え</a>には程遠いなぁ。恐ろしや。</p>

<h3>最後に</h3>

<p>運営の皆様、ベンチマークツールの<a href="http://isucon.net/archives/40434032.html">不具合</a>やインスタンスガチャの問題等ありましたが、対応方針は非常に納得のいくものでした。引き続き、本戦を楽しみにしています。ありがとうございました！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ConsulによるMySQLフェールオーバー]]></title>
    <link href="http://ijin.github.io/blog/2014/07/11/mysql-failover-with-consul/"/>
    <updated>2014-07-11T10:12:00+09:00</updated>
    <id>http://ijin.github.io/blog/2014/07/11/mysql-failover-with-consul</id>
    <content type="html"><![CDATA[<p>先日(6/22/14)、6月なのにどういう分けか早めに開催された<a href="http://2014.techfesta.jp/">July Tech Festa 2014</a>でConsulについて<a href="http://2014.techfesta.jp/p/program.html?m=1#c43">発表</a>してきた。そのユースケースの一つとしてMySQL failoverをちょっとだけ紹介したので、ここに詳しく書いておく。</p>

<h2>MHA</h2>

<p>MySQLレプリケーションの障害時にフェールオーバーしたい場合、MHAを使うの結構ポピュラー（日本では）だと思います。MHAは最新binlogの適用、Slaveの昇格とレプリケーションの張替えまではやってくれますが、実際のフェールオーバーの部分はユーザに委ねられていて、master_ip_failover_scriptのテンプレートをカスタマイズするか独自実装する必要があり、一般的な実現方法としてはカタログデータベースの更新かVirtual IPの切替等があります。</p>

<p>Virtual IPだと居残りセッションの問題や切替の保証難しかったり、そもそも環境によっては使えなかったりするので、私はあんまり好きじゃありません。また以前、後者的アプローチとしてAWSのVPC内であればrouting tableを変更する事によってこの挙動に似た実現方法を<a href="http://ijin.github.io/blog/2013/05/21/custom-non-rds-multi-az-mysql-replication/">紹介</a>した事がありますが、一番の問題点はAPI backplaneがSPoFになってて、ここが落ちたらそもそも動かない；また、APIのrate limitに達して呼び出しさえ出来ないという結構痛い目に会ったりします。</p>

<p>そこで前者的なアプローチとしてmasterの情報を管理するカタログデータベースの更新部分にConsulを使ってみました。</p>

<h2>CONSUL</h2>

<p>ConsulとはHTTP APIとDNSで操作ができる分散型クラスタで、VagrantやSerf等を開発しているHashicorpの新プロダクトです。Key featureは以下のとおり。</p>

<ul>
<li>Service Discovery</li>
<li>Failure Detection</li>
<li>Multi Datacenter</li>
<li>Key/Value Store</li>
</ul>


<h3>ConsulのConsistencyについて</h3>

<p>ConsulはCAP定理でいうCPという特性をもっており、Consistency（一貫性）に重きをおいてあります。Paxosを由来とするRaftをベースにしたconsensus protocolで実現していて、ピアセット内の各サーバノードでlog entryの書き込みがquorumで決定された後にcommitされたと見なされ、FSMに書き込まれます。よって、書き込みに関してはStrongly Consistentな処理となります。読み込みに関しては、パフォーマンス要求に応じてConsistencyレベルをクエリータイプによって調整可能（まるでCassandraのように！）で、Usually Consistent, Strongly Consistent, Staleから選択可能です。DNSベースのクエリーはデフォルトで単一のリーダーノードが返答するのでStrongly Consistentな処理となっています。（Staleにする事も可能）</p>

<h2>ConsulとMHAの連携概要</h2>

<ul>
<li>Consulを内部DNSとして使い、clientはDNSベースでmasterに接続</li>
<li>MySQL masterはAPIで予めサービス名（alias DNS）を登録しておく</li>
<li>mysql_failover_scriptで旧情報の削除と新情報の登録をやる

<ul>
<li>旧master IPを無効化する部分でConsulのCatalog endpointを使ってderegister (/v1/catalog/deregister)</li>
<li>新master ipを登録する部分でConsulのCatalog endpointを使ってregister (/v1/catalog/register)</li>
<li>成功しない限り進まない（exit code check）</li>
</ul>
</li>
<li>削除、及び登録はconsulのconsistency modelによって一貫性は保証される</li>
</ul>


<h2>DEMO</h2>

<div class="embed-video-container"><iframe src="http://www.youtube.com/embed/rA4hyJ-pccU "></iframe></div>


<h2>Notes</h2>

<p>DNSは最新のv0.3.0になってからTTLを設定できるようになったので、Amazon RDSっぽい感じのフェールオーバーも可能ですが、v0.2系に比べて格段にパフォーマンスが向上（スライド参照）したので、デフォルトのTTL 0でも問題ない範囲になってる感じです。また、もうちょっと詳しい内容は今度の<a href="http://connpass.com/event/7322/">#hbstudy</a>で話す予定です。</p>

<h2>スライド</h2>

<p>以下、July Tech Festa 2014で発表した時のスライドです。</p>

<script async="true" class="speakerdeck-embed" data-id="e07317d0dc040131a0982229a5c1e016" src="http://ijin.github.io//speakerdeck.com/assets/embed.js"> </script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[#ChefConf 2014に参加してきた]]></title>
    <link href="http://ijin.github.io/blog/2014/04/21/chefconf-2014/"/>
    <updated>2014-04-21T12:10:00+09:00</updated>
    <id>http://ijin.github.io/blog/2014/04/21/chefconf-2014</id>
    <content type="html"><![CDATA[<p>San Franciscoで行われた<a href="http://chefconf.opscode.com/chefconf/">#ChefConf</a>に参加してきました。
忘れないうちに忘備録的に少しメモっておく。</p>

<h2>Day 1</h2>

<h3>Awesome Postmortems by Dave Zwieback</h3>

<p>システム障害に対して素晴らしいPost Mortem（振り返り／報告書）の書き方に関する丸一日のワークショップ</p>

<h4>前半</h4>

<p>まずはチームに分かれて断片的且つ関連性の不明な情報を渡される。
例えば、</p>

<ul>
<li>Tomは紫色の家の住人より短い</li>
<li>Jimは両隣の住人より高い</li>
<li>赤色の家の隣人は子供が5人いる</li>
</ul>


<p>各メンバーは情報を全部開示できないまま、ある不明確なタスクを時間内に完了させる必要がある。しかし、紙やモノを使って情報の整理をしてはならず、口頭による連絡のみなので当然情報は錯綜しタスクは未完のまま終了。</p>

<p>障害時の情報不足・体制不足のシミュレーション。Nosey Neighborsと言うゲームらしい。</p>

<p>その後、お怒りのCEOからDarth Vader風のオーディオ・メッセージで以下を解答せよとのお達しが。。</p>

<ul>
<li>根本原因は何か</li>
<li>何をすれば良かったか</li>
<li>誰の責任か</li>
</ul>


<p>メンバー間で議論して各グループとの比較・プレゼンする</p>

<h4>後半</h4>

<p>パラダイム・シフトとディスカッション形式</p>

<p>以下、メモ</p>

<ul>
<li>たった一つの根本原因は存在しない。システムはimpermanence（非恒久・無常）である。コレに対して誰かが「なんてZenなんだ！」w</li>
<li>そもそもHuman Error（人的エラー）は原因ではなくもっと大きな問題を示すサインであり、症状であり個人やグループのせいにしてはいけない。そうすると簡単で気持ち良いが解決にはならない。80年代の航空会社の事故分析のパラダイム・シフトを引き合いに</li>
<li>Accountability vs Responsibility（個人・グループに責任は負わせないけど説明・報告はさせるべき）</li>
<li>その為にもBlameless（誰かを責める事のないように）でNonpunitive（非罰則）にするべき。火災の消防活動に対する消防隊員の扱いの例が面白かった</li>
<li>Hindsight/Outcome Bias（事後だと情報が多くより物事がより鮮明になるけどそれは偏見である）</li>
<li>Counterfactual（事実に反する。タラレバ）な事を書いてはならない。起こった事は事象は変えられない</li>
<li>3 Rs (Regret, Reason, Remedy)　遺憾を示し、事象のリニアなタイムラインを記載し、解決案の列挙</li>
<li>5 Whys 日本では「なぜなぜ分析」というらしい。要因追求のイテレーティブ・プロセス。</li>
<li>Sharp End vs Blunt End（顕在的エラー vs 潜在的エラー）</li>
<li><a href="https://github.com/etsy/morgue">Morgue</a> - Postmortem用の便利なツール</li>
</ul>


<p>主に組織論やカルチャーの話やDevOpsとの関連性の議論等。今度<em>#トラしゅ</em>に組み込もうかな。</p>

<p>※ <em>山◯君はこっちではボブと呼ばれる</em></p>

<h2>Day 2</h2>

<h3>Keynote</h3>

<ul>
<li>ヘビメタ風のBGM</li>
<li>Barry Crist CEOによるDelightful Economyの熱いプレゼン。Uberの紹介</li>
<li><a href="https://github.com/opscode/chef-metal">Chef Metal</a>の紹介／デモ（DockerやMongoDBで）</li>
<li><a href="http://www.getchef.com/blog/2014/04/15/chef-development-kit/">ChefDK (Chef Development Kit)</a>の紹介</li>
</ul>


<h3>Hunting the DevOps Whale in Large Enterprises</h3>

<ul>
<li>大企業でのDevOpsの話</li>
<li>かなり抽象的でメタファー引用多数</li>
<li>Scrumfall (Scrum + Waterfall)という悪い冗談のような本当にあった怖い話</li>
</ul>


<h3>Spice up your recipes with Chef Sugar</h3>

<p>ChefSpec、Test Kitchen、BerkshelfのコアコミッターであるSeth Vargoによる<a href="https://github.com/sethvargo/chef-sugar"><strong>Chef Sugar</strong></a>の紹介。コードをよりrubyっぽく、より美しくする為のsyntax sugar</p>

<h3>The Berkshelf Vision</h3>

<ul>
<li><a href="http://berkshelf.com/">Berkshelf</a>の原作者であるJamie WinsorによるBerkshelf 3.0の紹介</li>
<li><a href="http://www.getchef.com/blog/2014/04/15/chef-development-kit/">ChefDK</a>でのインストールを推奨</li>
<li>推奨されるべき新しいワークフロー管理や手法について</li>
<li>その補助ツールである<a href="https://github.com/reset/berkflow">Berkflow</a>の説明</li>
<li>このセッションが一番面白かったのでyoutubeに上がったらまた見るべし！</li>
<li><a href="http://www.slideshare.net/resetexistence/chef-conf2014">スライド</a></li>
</ul>


<h3>Implementing Continuous Delivery in Chef</h3>

<p>継続デリバリーのお話。発表者がつまらなかったので、お仕事してた。</p>

<h3>Chef and Docker</h3>

<ul>
<li>Dockerの紹介やロードマップ</li>
<li>実装方法</li>
<li>Chefとの組み合わせ方</li>
<li>2014 2QのDockerConで1.0が発表されるかも</li>
<li>監視に関して突っ込んで質問したら、夏以降に出るであろう監視用コンテナに期待とな</li>
<li>ついでにTシャツもらった</li>
</ul>


<h3>BoF - Chef on AWS</h3>

<ul>
<li>AWS上でのChef利用に関してBoF (Birds of Feather)形式のディスカッション</li>
<li>Cloudformation、Autoscaling、Opsworks、Ohai等</li>
<li>特にOpsworksの使い勝手の悪さを熱く議論してきた</li>
<li>また、OhaiはIAM roleの情報をキャッシュするのでChef Serverとの組み合わせが悪い</li>
<li><a href="https://github.com/balanced-cookbooks/citadel">Citadel</a>というので回避してる人も</li>
</ul>


<h2>Day 3</h2>

<h3>Keynote</h3>

<p>Adam Jacob CTOによるChef社の歴史、思想や方向性やDevOps Cultureについて</p>

<h3>Get Up Again (Over and Over): Learning and Relearning with Chef</h3>

<p>変化への対応、リファクタリング、実験的コードの組み立て方等</p>

<h3>Foreman and Chef Integration</h3>

<p>Red HatによるForemanの発表。あんまり聞いてなかった</p>

<h3>DevOps Culture And Practices To Create Flow</h3>

<ul>
<li>ThoughtworksのJez Humbleによる発表</li>
<li>自動化や継続デリバリーによるリーンな開発手法</li>
<li>トヨタやHPの事例</li>
<li>社内カルチャーの話</li>
<li>Jesse&#8217;s rule - &#8220;Don&#8217;t fight stupid. Make awesome&#8221;</li>
<li>なかなか良いスピーカーであった</li>
</ul>


<h3>その他</h3>

<ul>
<li>アメリカのクライアントやリモートでの仕事仲間と初顔合わせ</li>
<li>ランチで会った人は同じくCassandra構築に苦労してて盛り上がった</li>
<li>Chef Zeroの作者と会って中の動きについてお話した</li>
<li>mizzy氏を発見。立ち話を少々</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS Game Day Japan 2014春を開催してきた]]></title>
    <link href="http://ijin.github.io/blog/2014/03/14/aws-game-day-japan-2014-spring/"/>
    <updated>2014-03-14T23:11:00+09:00</updated>
    <id>http://ijin.github.io/blog/2014/03/14/aws-game-day-japan-2014-spring</id>
    <content type="html"><![CDATA[<p>過去に2度参加した事（東京・ラスベガス）がある<a href="http://ijin.github.io/blog/2013/06/10/aws-game-day-tokyo-2013/">Game Day</a>に今回は運営側に周りました。3/15に行われる<a href="http://jawsdays2014.jaws-ug.jp/">JAWS Days 2014</a>の前夜祭という位置づけです。</p>

<p>日本では前回、東京のみだったけど<a href="http://jaws-days.doorkeeper.jp/events/8945">今回</a>は東京・大阪・名古屋・仙台と4都市同時開催。</p>

<h2>お題</h2>

<p>前回と全く一緒。。SQSを使った疎結合でオートスケーリングする画像変換処理システム（ｻｰｾﾝ）。まあ、1年前と比べてAWSの機能やできる事も大分変わったので2回目の人もいろいろ工夫のしようがあったかと。</p>

<h2>チーム</h2>

<p>1チーム3〜4人に別れて計14チームとなりました。</p>

<p><strong>東京</strong></p>

<ul>
<li>Cookie Devil</li>
<li>Bluescreens</li>
<li>沖縄</li>
<li>Blue Light of Death</li>
<li>時計じかけのオレンジ</li>
<li>I am みどり</li>
</ul>


<p><strong>名古屋</strong></p>

<ul>
<li>Shachihoko</li>
<li>ななちゃんだがや</li>
<li>ゴーゴーひつまぶし</li>
</ul>


<p><strong>大阪</strong></p>

<ul>
<li>大都会</li>
<li>AWS学坊や</li>
<li>初心者</li>
</ul>


<p><strong>仙台</strong></p>

<ul>
<li>Zao</li>
<li>八重の桜</li>
</ul>


<h2>内容</h2>

<p>当日の迫力ある詳しい内容は参加者の方がきっとブログに書いてくれるはず！その代わり、運営側で評価用に使っていた攻撃・修復内容のまとめを公開して欲しいというツィートを頂いたので下記に表示しておきます。</p>

<div class='embed tweet'><blockquote class="twitter-tweet"><p>Gameday楽しめました。運営ありがとうございました。攻撃のサマリシート翻訳は誰?と思ってましたが恐縮です。あのシート何らかの形で公開頂けるとありがたいです。 QT <a href="https://twitter.com/ijin">@ijin</a>: 14チームのGoogleスプレッドシートのリアルタイム翻訳は結構ギリギリだった。</p>&mdash; Ryo Suzuki (@suzryo) <a href="https://twitter.com/suzryo/statuses/444450014627000320">March 14, 2014</a></blockquote>
<script async src="http://ijin.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script></div>


<p></p>

<p>（実は今回もイベントオーナーのMiles Wardが来日していたので、Google Spreadsheetに書き込まれていた内容を運営側で逐一翻訳していました）</p>

<iframe src="https://docs.google.com/spreadsheets/d/16-qSbw_XQGgl_4bYqy9REL-nq15yOVj2f1HBJvMy_qM/pubhtml?widget=true&amp;headers=false" height="500" width="750"></iframe>


<p>個人的にはs3の「Requester Payオプション有効化」がお気に入りでしたね。</p>

<h2>結果</h2>

<p>総合優勝はチーム<strong>「AWS学坊や」</strong>の優勝となりました。おめでとうございます！
（地方賞はすみません、総合結果評議中で聞けてませんでした）</p>

<p>今回、運営側のトラブルで攻撃対象が重複してしまう等いろいろ問題がありましたが、開催する側としても楽しかったです。次回はさらにチャレンジングな内容にしていきたいと思っています。</p>

<h2>JAWS DAYS</h2>

<p>明日の<a href="http://jawsdays2014.jaws-ug.jp/">JAWS DAYS 2014</a>はスタッフとしていろいろ動くので今回は手短に。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[i2 instanceでMySQLベンチマーク]]></title>
    <link href="http://ijin.github.io/blog/2014/01/01/mysql-benchmarks-on-aws-i2-instance-ssds/"/>
    <updated>2014-01-01T00:00:00+09:00</updated>
    <id>http://ijin.github.io/blog/2014/01/01/mysql-benchmarks-on-aws-i2-instance-ssds</id>
    <content type="html"><![CDATA[<p>新年明けました。おめでとうございます。</p>

<p>すっかり12月の<a href="http://www.zusaar.com/event/1117005">aws</a>/<a href="http://www.zusaar.com/event/1847003">mysql</a> advent calendarに乗り遅れたので、AWSのi2 instanceでのMySQLのベンチマークを勝手におまけとして公表します。
以前取ったhi1.4xlargeとの比較になります。</p>

<h2>構築</h2>

<p>SSDディスクはAWSが<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/i2-instances.html#i2-instances-diskperf">推奨する</a>over-provisioningを有効にする為に10%を非partitionし、それぞれ720Gでパーティション作成。</p>

<pre><code>sudo mdadm --create /dev/md0 --level 0 --raid-devices 8 /dev/xvdb1 /dev/xvdc1  /dev/xvdd1 /dev/xvde1 /dev/xvdf1 /dev/xvdg1 /dev/xvdh1 /dev/xvdi1
sudo mkfs.xfs -f -b size=4096 -i size=512 -l size=64m /dev/md0
sudo mount -t xfs -o noatime,logbufs=8 /dev/md0 /data
</code></pre>

<p>OSはkernel versionが3.8以上が<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/i2-instances.html#i2-instances-diskperf">望ましい</a>のでいつものLTSではなく、Ubuntu Server 13.10のHVMタイプ (ami-b93264d0)</p>

<h2>sysbench</h2>

<p>やり方やパラメータは<a href="http://ijin.github.io/blog/2013/02/22/mysql-benchmarks-on-aws-ssd-vs-fusion-io/">前回</a>の計測方法と同じ。i2.8xlaregは32coreですが、今回はhi1.4xlargeの時と比較する為に敢えて16スレッドで計測しました。</p>

<ul>
<li>sysbenchのoltpモード</li>
<li>データサイズは12G（5000万件）</li>
<li>readonly</li>
<li>uniform（フルスキャン）</li>
</ul>


<p>コマンド</p>

<pre><code>time sysbench --test=oltp --oltp-table-size=50000000 --db-driver=mysql --mysql-user=root --num-threads=16 --max-requests=0 --max-time=180 --init-rng=on --oltp-read-only=on --oltp-dist-type=uniform 2&gt;&amp;1 run
</code></pre>

<p>トランザクション推移</p>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&oid=5&zx=jwv2ytp6xwx3"></p>

<p>レスポンスタイム推移</p>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&oid=6&zx=sfl6ocblw5ob"></p>

<p>速いですね。</p>

<h2>tpcc-mysql</h2>

<p>こちらも<a href="http://ijin.github.io/blog/2013/02/22/mysql-benchmarks-on-aws-ssd-vs-fusion-io/">前回</a>)の計測方法と同じ。</p>

<ul>
<li>500 warehouses (50GBぐらい)</li>
<li>24GB Buffer pool</li>
<li>16スレッド</li>
<li>1時間実行</li>
</ul>


<p>コマンド</p>

<pre><code> tpcc_load localhost tpcc root "" 500
 tpcc_start -d tpcc -u root -p "" -w 500 -c 16 -r 300 -l 3600
</code></pre>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&oid=7&zx=fc6nz8iel3ez"></p>

<p>hi1.4xlargeはSSD1台で計測した事を考えると、同価格帯のi2.4xlarge（SSD4台）の半分（SSD2台）のパフォーマンスが出るのは妥当ですね。</p>

<h2>fio</h2>

<p>ついでにfioでそれぞれRAID0した場合のベンチマークも取ってみたけど、<a href="http://d.hatena.ne.jp/rx7/20131224/p1">並河さん</a>と結果が違ってwriteがスケールしてます。OSとmkfs.xfsのオプションしか違わないはずだけど。。</p>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&oid=9&zx=v7nbywda04bp"></p>

<h2>その他</h2>

<p>いやー。spot instanceがないので計測だけで結構かかってしまった。</p>

<p>しかし、なんでhi1世代の次はhi2ではなく、i2なんだろう。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Autoscalingによる自動復旧(Immutable Infrastucture)]]></title>
    <link href="http://ijin.github.io/blog/2013/12/14/self-healing-with-non-elb-autoscaling2/"/>
    <updated>2013-12-14T23:40:00+09:00</updated>
    <id>http://ijin.github.io/blog/2013/12/14/self-healing-with-non-elb-autoscaling2</id>
    <content type="html"><![CDATA[<p>以前、「<a href="http://ijin.github.io/blog/2013/02/08/self-healing-with-non-elb-autoscaling/">非ELBなAutoscalingによる自動復旧</a>」でインスタンスの自動復旧の挙動をテストしました。
障害が発生したサーバをterminateし、新サーバをstartしてリプレースする仕組みはまさに最近話題のImmutable Infrastructureですね。
CDP的には「<a href="http://aws.clouddesignpattern.org/index.php/CDP:Server_Swapping%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3">Server Swappingパターン</a>」が一番近いですが、今後はImmutable分類もあっても良いような気がします。</p>

<p>前回はAuto Scalingがインスタンス障害を検知してリプレースするまでのタイムラグが約20分だと分かりました。
本日、インスタンスの状態をチェックするEC2 Status Checkが1分間隔になった（以前は5分間隔）と<a href="https://forums.aws.amazon.com/ann.jspa?annID=2266">発表</a>されたので、
これによってタイムラグが短縮されたかを検証してみます。</p>

<h3>設定</h3>

<p>手順は前回と一緒なので省略</p>

<h3>自動復旧</h3>

<p>通信を遮断し、Status Check Failを発動させる</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ubuntu@ip-10-123-32-180:~$ date; sudo ifdown eth0
</span><span class='line'>Sat Dec 14 14:19:19 UTC 2013
</span><span class='line'>Write failed: Broken pipe</span></code></pre></td></tr></table></div></figure>


<p>EC2のStatus Checkを流す</p>

<pre><code>while true; do date; aws ec2 describe-instance-status --instance-ids i-b03788b5 --query 'InstanceStatuses[*].InstanceStatus' --output text ; echo ; sleep 10; done
</code></pre>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Sat Dec 14 23:22:05 JST 2013
</span><span class='line'>ok
</span><span class='line'>DETAILS reachability    passed
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:22:16 JST 2013
</span><span class='line'>ok
</span><span class='line'>DETAILS reachability    passed
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:22:27 JST 2013
</span><span class='line'>impaired
</span><span class='line'>DETAILS 2013-12-14T14:22:00.000Z        reachability    failed
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:22:37 JST 2013
</span><span class='line'>impaired
</span><span class='line'>DETAILS 2013-12-14T14:22:00.000Z        reachability    failed</span></code></pre></td></tr></table></div></figure>


<p>約3分でStatus異常が検知されました。</p>

<p><img src="https://lh5.googleusercontent.com/-pabfvBU5fW0/Uqx4SEmBYLI/AAAAAAAAA2E/abCLUwoESds/w734-h154-no/Instance_status_check_2013-12-14+at+11.34.27+PM.png"></p>

<p>Auto ScalingのHealthStatusを流す</p>

<pre><code>while true; do date; aws autoscaling describe-auto-scaling-instances --query 'AutoScalingInstances[*].HealthStatus' --output text; echo; sleep 10; done
</code></pre>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Sat Dec 14 23:38:06 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "InService", 
</span><span class='line'>        "Health": "HEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:38:17 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "InService", 
</span><span class='line'>        "Health": "UNHEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:38:28 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "InService", 
</span><span class='line'>        "Health": "UNHEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:38:38 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "Terminating", 
</span><span class='line'>        "Health": "UNHEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:38:49 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "Terminating", 
</span><span class='line'>        "Health": "UNHEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:38:59 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "Terminating", 
</span><span class='line'>        "Health": "UNHEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:39:10 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "Pending", 
</span><span class='line'>        "Health": "HEALTHY", 
</span><span class='line'>        "ID": "i-4cc7a849"
</span><span class='line'>    }, 
</span><span class='line'>    {
</span><span class='line'>        "State": "Terminating", 
</span><span class='line'>        "Health": "UNHEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span></code></pre></td></tr></table></div></figure>


<p>やっとAuto Scalingの方でも異常検知。</p>

<p><img src="https://lh4.googleusercontent.com/-8Dj_s0mm9I8/Uqx4R1-Wl9I/AAAAAAAAA2I/laUZmwhUw8o/w873-h151-no/Autoscaling_health_2013-12-14+at+11.57.42+PM.png"></p>

<p>SNS通知</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Service: AWS Auto Scaling
</span><span class='line'>Time: 2013-12-14T14:39:41.271Z
</span><span class='line'>RequestId: f622c6d2-8c77-4fef-8b38-ece463574712
</span><span class='line'>Event: autoscaling:EC2_INSTANCE_TERMINATE
</span><span class='line'>AccountId: 111155559999
</span><span class='line'>AutoScalingGroupName: test-sg
</span><span class='line'>AutoScalingGroupARN: arn:aws:autoscaling:ap-northeast-1:11115559999:autoScalingGroup:0e771015-f979-4afe-b065-595abafdbf9b:autoScalingGroupName/test-sg
</span><span class='line'>ActivityId: f622c6d2-8c77-4fef-8b38-ece463574712
</span><span class='line'>Description: Terminating EC2 instance: i-b03788b5
</span><span class='line'>Cause: At 2013-12-14T14:38:38Z an instance was taken out of service in response to a system health-check.
</span><span class='line'>StartTime: 2013-12-14T14:38:38.257Z
</span><span class='line'>EndTime: 2013-12-14T14:39:41.271Z
</span><span class='line'>StatusCode: InProgress
</span><span class='line'>StatusMessage:
</span><span class='line'>Progress: 50
</span><span class='line'>EC2InstanceId: i-b03788b5</span></code></pre></td></tr></table></div></figure>


<p>やはり20分のタイムラグ変わらずですね。。</p>

<h3>結論</h3>

<p>というわけで、EC2 Status Checkが1分間隔になっても、EC2のみ（ELBを使わい場合）のAuto Scalingによる不調インスタンスの自動復旧時間は変わらずでした。</p>

<p>ちなみにAWS ConsoleでAuto Scalingの設定ができるようになったけど、まだscaling groupにtagが付けられないのがちょっと微妙ですね。。GUIで状態を見る分には楽だけど。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[サバフェス！2013に参加してきた]]></title>
    <link href="http://ijin.github.io/blog/2013/12/13/serverfesta-2013-autumn/"/>
    <updated>2013-12-13T15:59:00+09:00</updated>
    <id>http://ijin.github.io/blog/2013/12/13/serverfesta-2013-autumn</id>
    <content type="html"><![CDATA[<p>少し前に<a href="http://connpass.com/event/3690/">サバフェス！2013 Autumn</a>に参加してきました。
内容を忘れないうちにやった事を書いておきます。</p>

<p>スコアはトップレベルだったものの、運営側がサーバを起動した所、自動でサービスが立ち上がらなかったらしいので残念な事に参考値のみに。（提出前に2回ぐらい再起動確認したのにおかしいなぁ。。）
優勝スコアは90.830 (GET 199,802 : POST 18,106)で、私のは<strong>93.410</strong> (GET 162,923 : POST 29,930)でした。</p>

<h2>お題</h2>

<p>「最速インフラを構築せよ！！！</p>

<p>WordPressに一切手を加えずに、どこまで高速化できるのか！？</p>

<p>OSチューニング、サーバチューニング、負荷分散…最適解を探せ！」</p>

<p>&#8211;</p>

<p><a href="http://www.idcf.jp/cloud/service/self.html">IDCFクラウド</a>上で仮想マシン5台（M8タイプまで）を使ってスコアを競うというもの。
<a href="https://www.facebook.com/tuningathon">チューニンガソン</a>と似てますが、サーバが複数台使えるのが良いですね。</p>

<h2>構成</h2>

<p>表彰式でLTしたけど、LVS (DSR) + php 5.5 + apcu + Varnish + nginx (lua) + memcached</p>

<p>ポイントは</p>

<ul>
<li>GET時は1台では帯域の限界に達したのでLVS (DSR)による4台での並列応答</li>
<li>POST時には必ず各Varnishのキャッシュクリア（ban）する</li>
<li>POST時にnginx->memcachedへ渡すと高速すぎたのであえてsleepを5msして遅延させる</li>
<li>nginx(lua)はコード量が多いと通らないのでぎりぎりまで削減</li>
<li>memcachedからmysqlへの非同期処理（5ms間隔）</li>
<li>mysqlの更新処理はそんなにいらないので基本チューニングとInnoDBにしただけで、5.1のまま</li>
<li>サーバはM8までいらないのでM4で</li>
<li>海外から参戦したけど、GUIが重いのでAPI経由での操作</li>
</ul>


<p>でしょうか。特に意識したのはmemcachedからmysqlへの許容範囲内での同期と複数台あるVarnishのキャッシュクリアですね。他のチームはnginxでTTLを設定して逃げたようですが、実運用時にはPOST時にキャッシュクリアを確実にする必要があるのでVarnish moduleをコンパイルして他のsiblingへ並列でban処理を投げてました。まあ、今回のベンチマークツールはそこまで厳格じゃなかったけど、最終チェックは人間が動作させるので。</p>

<p>LT資料。スコア推移と簡単な構成の紹介</p>

<div class="embed-ss-container"><iframe src="http://www.slideshare.net/slideshow/embed_code/29171459 "></iframe></div>


<h2>設定ファイル</h2>

<p>以下、設定ファイルです。いらない所は削ったけど動くはず。。</p>

<p>backend varnish vcl （各backendの設定は微妙に違う）:</p>

<div><script src='https://gist.github.com/7941009.js?file='></script>
<noscript><pre><code>&lt;html&gt;&lt;body&gt;You are being &lt;a href=&quot;https://github.com/gist/7941009&quot;&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;</code></pre></noscript></div>


<p>backend nginx.conf:</p>

<div><script src='https://gist.github.com/7940999.js?file='></script>
<noscript><pre><code>&lt;html&gt;&lt;body&gt;You are being &lt;a href=&quot;https://github.com/gist/7940999&quot;&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;</code></pre></noscript></div>


<p>base nginx.conf:</p>

<div><script src='https://gist.github.com/7941214.js?file='></script>
<noscript><pre><code>&lt;html&gt;&lt;body&gt;You are being &lt;a href=&quot;https://github.com/gist/7941214&quot;&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;</code></pre></noscript></div>


<p>base syncer.rb:</p>

<div><script src='https://gist.github.com/7941042.js?file='></script>
<noscript><pre><code>&lt;html&gt;&lt;body&gt;You are being &lt;a href=&quot;https://github.com/gist/7941042&quot;&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;</code></pre></noscript></div>


<p>base supervisord.conf:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[program:syncer]
</span><span class='line'>command=/home/mho/.rvm/bin/ruby /home/mho/syncer/sync.rb 5
</span><span class='line'>stdout_logfile_maxbytes=1MB
</span><span class='line'>stderr_logfile_maxbytes=1MB
</span><span class='line'>stdout_logfile=/tmp/%(program_name)s-stdout.log
</span><span class='line'>stderr_logfile=/tmp/%(program_name)s-stderr.log
</span><span class='line'>user=mho
</span><span class='line'>directory=/home/mho/syncer
</span><span class='line'>autostart=true
</span><span class='line'>autorestart=true</span></code></pre></td></tr></table></div></figure>


<p>base my.cnf:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[mysqld]
</span><span class='line'>datadir=/var/lib/mysql
</span><span class='line'>socket=/var/lib/mysql/mysql.sock
</span><span class='line'>user=mysql
</span><span class='line'># Disabling symbolic-links is recommended to prevent assorted security risks
</span><span class='line'>symbolic-links=0
</span><span class='line'>character-set-server = utf8
</span><span class='line'>max_connections = 1000
</span><span class='line'> 
</span><span class='line'>key_buffer_size = 32M
</span><span class='line'>max_allowed_packet = 16M
</span><span class='line'>thread_stack = 192K
</span><span class='line'>thread_cache_size  = 200
</span><span class='line'> 
</span><span class='line'>#slow_query_log=1
</span><span class='line'>#long_query_time=0
</span><span class='line'>query_cache_type = 0
</span><span class='line'>skip-innodb_doublewrite
</span><span class='line'>innodb_buffer_pool_size = 192M
</span><span class='line'>innodb_log_buffer_size = 4M
</span><span class='line'>innodb_flush_log_at_trx_commit = 0
</span><span class='line'>innodb_support_xa = 0</span></code></pre></td></tr></table></div></figure>


<p>sysctl.conf:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>fs.file-max = 1048576
</span><span class='line'>
</span><span class='line'>net.ipv4.ip_local_port_range = 1024 65535
</span><span class='line'>
</span><span class='line'>net.core.wmem_max = 16777216
</span><span class='line'>net.core.rmem_max = 16777216
</span><span class='line'>
</span><span class='line'>net.ipv4.tcp_wmem = 4096 65536 16777216
</span><span class='line'>net.ipv4.tcp_rmem = 4096 87380 16777216
</span><span class='line'>
</span><span class='line'>net.core.somaxconn = 8192
</span><span class='line'>
</span><span class='line'>net.core.netdev_max_backlog = 8000
</span><span class='line'>
</span><span class='line'>net.ipv4.tcp_max_syn_backlog = 8192
</span><span class='line'>
</span><span class='line'>net.ipv4.tcp_synack_retries = 3
</span><span class='line'>net.ipv4.tcp_retries2 = 5
</span><span class='line'>
</span><span class='line'>net.ipv4.tcp_keepalive_time = 900
</span><span class='line'>
</span><span class='line'>net.ipv4.tcp_keepalive_probes = 3
</span><span class='line'>
</span><span class='line'>net.ipv4.tcp_keepalive_intvl = 15
</span><span class='line'>
</span><span class='line'>net.nf_conntrack_max = 1000000</span></code></pre></td></tr></table></div></figure>


<h2>終わりに</h2>

<p>最初はecho選手権になってたのであんまりやる気がなかったけど、後半はちょっと楽しめました。結果は惜しかったけど、今まで<a href="https://www.facebook.com/tuningathon">チューニンガソン</a>や<a href="http://isucon.net">ISUCON</a>に出場したり、トラブル☆しゅーたーずを主催したり、日々の運用等の経験が生きた感じがします。運営の皆様、ありがとうございました。次があれば楽しみにしています！（レギュレーションの解釈が微妙だったのでそこはブラッシュアップして欲しいですね）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ISUCON3の本戦に参加してきた]]></title>
    <link href="http://ijin.github.io/blog/2013/11/11/isucon3-final/"/>
    <updated>2013-11-11T11:27:00+09:00</updated>
    <id>http://ijin.github.io/blog/2013/11/11/isucon3-final</id>
    <content type="html"><![CDATA[<p>先月の<a href="http://ijin.github.io/blog/2013/10/07/isucon3-preliminary/">予選に通過したの</a>で、<a href="http://isucon.net/">ISUCON3</a>の本戦に参加してきました。</p>

<p>完敗。</p>

<h3>お題発表</h3>

<p>画像版twitter。投稿する画像の公開レベルをpublic, private, followers onlyに設定できるシステムが1台のVPS（2コア、4GB RAM）で動いている状態。プレスを打った為、大量アクセスがくる18時までに別途用意された4台のサーバを使ってスケールせよという使命を与えられる。</p>

<h3>流れ</h3>

<p>画像データが1万点・約3GBあったので、まず失敗しても戻れるようにバックアップ取得を開始。それと平行して他のサーバへのsshキー登録したり、hosts書いたり、もろもろ下準備。</p>

<p>デフォルトのperlのスコアは<strong>1206.8</strong></p>

<p>言語はrubyと決めていたので、supervisorで立ち上げてみるが起動失敗。よくよく調べてみるとforemanが入ってなくてGemfileに追加してbundle。</p>

<p>この時のスコアが<strong>1180.8</strong></p>

<p>次にデータベースを見てみるものの、レコード数も比較的少なく、総容量が2MBもないのでできる事は限定されていると判断。クエリーをさらっと見た後にentriesのimageカラムに対してインデックスを張ったぐらい。</p>

<p>アクセスログにレスポンスタイムを出力するようにして1回ベンチを走らせログを解析。</p>

<p>ブラウザ上の挙動を確認しつつ、ソースコードを読んで結局画像変換のconvert処理が一番重そうだったのでそこから着手することに。</p>

<p>予選の時も外部プログラムを呼んでいるところが改善ポイントの一つだったので、まずfastimage_resizeを使って置き換えてみるものの、処理速度はそんなに上がらず、スコアもほぼ横ばい。</p>

<p>その間に、ロングポーリングの処理を変更してみるけど、</p>

<pre><code> "message": "2013-11-09T14:48:17 [36898] [CRITICAL] timeline refrection timeout"
</code></pre>

<p>タイムラインの反映がうまくいってない模様。
（ちなみにエラーメッセージのrefrectionはreflectionのスペルミスですね）</p>

<p>次に画像変換処理の部分で毎回リクエストがくる度に実行されるリンクをredisにてキャッシュ。これは効果があり、スコアは<strong>6634.2</strong>で暫定3位。</p>

<p>その間にVarnishやHaproxy + nfsを軽ーく試してみるものの、スコアは伸びず。</p>

<p>この辺でリンクだけではなく、画像自体をredisに突っ込んで全サーバで処理するアーキテクチャを決定。<a href="https://twitter.com/acidlemon">@acidlemon</a>さんと似た<a href="http://beatsync.net/main/log20131110.html">構成</a>ですね。ただ違うのはPOST後のsidekiqを使って処理を裏のワーカーに任せるという事。</p>

<p>sidekiqが動作するところまではでき、全画像の変換を試みるがredisサーバのメモリが溢れてたので、最初にアクセスされる直近30件、アクセス比率が高いサイズsと、新規画像のみに注力。スコアは徐々に上がる。</p>

<p>その後はただひたすらに、もくもく実装とデバッグ。</p>

<p>残り3-40分ぐらいのところで、生ハムチームでブレークスルーが起こり、彼らが一気にトップへ踊り出る。我々も1台構成であれば5位ぐらいにはなれただろうけど、スケールアウトしなければ全く勝負にならないので最後の最後まで果敢に挑戦するもあえなくタイムアップ。</p>

<p>結果、FAIL。</p>

<h3>感想</h3>

<p>今回はサーバが5台もあったので、スケールアウトしなければならないのは明白で、実装を真っ先に着手するべきでしたね。前半で1台だけチューニングして後でスケールしようと思ったのが戦略上の致命的ミス。時間切れで終わったので実装が間に合っていたらそれなりのスコアが出たはずかと。優勝した生ハムチームが結構ギリギリまでかかったのを考えると、やはり見極めたポイントは重要で、さすがとしか言いようがないです。また、一番時間のかかった画像配信に関しては普段AWSを使っている身としてはs3へ画像を突っ込むのが当然だと考えていたので、なかなか新鮮で違う脳を使う感じで楽しめました。</p>

<h3>その他</h3>

<ul>
<li>途中ディスカッションをすれば良かった（予選は上々でも本戦で焦ってしまった）</li>
<li>落ち着いて俯瞰して見るべし。見極め大事</li>
<li>予選とかの先入観が邪魔したのでまっさらの状態で考えるべき</li>
<li>ベンチマークツールはFAILしても再実行に2分待たされるのがどうしてももどかしかった</li>
<li>ベンチマークツールの他のサーバへの実行切替がバグってて時間をロスった</li>
<li>チーム名のRevengeが果たせなかったので、来年はチーム名どうしようかな。。</li>
<li>ドヤモリスが満面の笑みで幸せそうだった</li>
</ul>


<p>ISUCONのレベルも毎回毎回レベルが上がっていき、運営側の苦労が伺えます。本当にお疲れ様でした！また来年にも期待しています！</p>

<h3>おまけ</h3>

<p>さて。1年間待ち望んだイベントがあっという間に終わってしまって消失感・焦燥感を味わいつつも、気を取り直して次はAWS re:InventのGAMEDAYに日本から唯一（多分）の参加者として参戦します！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ISUCON3の予選を通過した（はず）]]></title>
    <link href="http://ijin.github.io/blog/2013/10/07/isucon3-preliminary/"/>
    <updated>2013-10-07T12:12:00+09:00</updated>
    <id>http://ijin.github.io/blog/2013/10/07/isucon3-preliminary</id>
    <content type="html"><![CDATA[<p><a href="http://ijin.github.io/blog/2012/11/05/isucon2/">去年</a>に引き続き参加した<a href="http://isucon.net/">ISUCON</a>第3回目の予選を通過しました。</p>

<p><a href="http://isucon.net/archives/32848462.html">結果</a>は参戦1日目の方で<strong>4位</strong>。
74チーム中、総合<strong>6位</strong>になりました。</p>

<h3>事前準備</h3>

<p>再び<a href="https://twitter.com/cads">@cads</a>と<a href="https://twitter.com/fruwe">@fruwe</a>に声をかけ、去年3位という雪辱を果たすべく、「The Revenge of Mr. Frank &amp; Co.」とチーム名を改名。
今年はチーム数が膨れると予想されてたので去年と違って予選がありトップ20位ぐらいまでしか本戦に行けないという仕組み。方針は以下の通り。</p>

<ul>
<li>言語は我々の得意なruby（goも面白そうだったけど）で勝負</li>
<li>去年Railsの再実装で失敗した為、Sinatra（であれば）でやりきる</li>
<li>Sinatra、Varnish、Sidekiq、Redisの復習</li>
<li>計測ツールの導入</li>
<li>早い時期に全体のスケジュールを決定する</li>
<li>戦略転換の見極め</li>
</ul>


<p>特に去年の敗因が戦略転換の遅れだったので、それを意識するようにしました。</p>

<h3>お題発表</h3>

<p>Markdownを使ったコメントシステム。しかもユーザログインがあってポストのprivate/public指定ができるというちょっとアドバンストな作り。</p>

<h3>前半戦</h3>

<p>今回、サーバはAWSのAMIとして提供され自分のアカウントで起動するやり方だったので、<a href="http://d.conma.me/entry/2013/04/08/190229">CPUガチャ</a>に若干期待しつつ10台一気にlaunchする。確認したら全部同じCPUだったので半分落として、後は本番用、開発用、バックアップ用として残す。</p>

<p>まずは、状況把握の為にひとまず皆でルールの理解とソース解析。ざっと目を通した後はブラウザで動作確認し、今回同一マシン内で実行させるベンチマークプログラムを複数言語で実行。制御方法は去年と同じsupervisord</p>

<p>Perl</p>

<pre><code>2013/10/05 10:31:15 done benchmark
Result:   SUCCESS
RawScore: 1020.4
Fails:    0
Score:    1020.4
</code></pre>

<p>Ruby</p>

<pre><code>2013/10/05 10:38:11 done benchmark
Result:   SUCCESS
RawScore: 2446.9
Fails:    0
Score:    2446.9
</code></pre>

<p>Go</p>

<pre><code>2013/10/05 10:45:32 done benchmark
Result:   SUCCESS
RawScore: 2840.1
Fails:    0
Score:    2840.1
</code></pre>

<p>Node</p>

<pre><code>2013/10/05 10:49:07 done benchmark
Result:   SUCCESS
RawScore: 1543.4
Fails:    0
Score:    1543.4
</code></pre>

<p>お、rubyそんなに悪くないかも。
デフォルトのperlが遅いのはメモリが温まってないかなと思って再実行したけど飛躍的に良くはならなかったですね。</p>

<p>この辺で予め決めていた開始後1時間という期限になったので作戦会議とスケジュール策定に入る。</p>

<p>まずはVarnishを入れてみようという事なので一人がその作業にとりかかる。もう一人はNewrelicという測定ツールの導入とバックアッププランのRedisバージョンの作りこみ。私はDB周りのチューニングやミドルウェア周り、後は二人のサポート。</p>

<p>DBはMySQLだったので全クエリを吐き出してpt-query-digestで解析して統計的に遅いクエリ順に潰す。
schemaをよく見たらindexがなかったので張ったり、クエリーをちょっと改善してみたり、パラメータを変更（innodb_buffer_pool_sizeとinnodb_flush_log_at_trx_commitのみ）したり黙々作業。</p>

<p>時間はあっという間に過ぎて両方のバージョンを導入してみるもののベンチマークツールからFAILを大量にくらい、スコアが全然つかず。。。</p>

<p>折り返し時点が迫ってきたので、とりあえず今あるチューニングだけのスコアを送信。</p>

<pre><code>2013/10/05 13:43:37 done benchmark
Result:   SUCCESS
RawScore: 3791.5
Fails:    6
Score:    3450.2
</code></pre>

<h2>後半戦</h2>

<p>その後、unicornのワーカー数を調整したり、クエリーを更に見なおしたり、initializer作って事前にキャッシュを温める仕組みを作ったりして6000ぐらいのスコアを出した気がする（この辺は作業ログを残すのを忘れていたので記憶が曖昧）</p>

<p>Redisバージョンは雰囲気的に無理そうだったので早々に捨ててこの辺は去年の教訓が生かされてると思う）ESI対応 + Varnishに注力する事を決断。しかし、こっちもエラーの連発。。</p>

<p>最も悩まされたのは</p>

<pre><code>14:53:20 [FAIL] invalid Cache-Control header
14:53:21 [FAIL] invalid post memo URL http://localhost/
</code></pre>

<p>どうやらリバースプロクシ用に付与しているヘッダーチェックで弾かれていた模様。</p>

<p>VCLファイルをよく見たら既存のシステムから流用したモノらしく、余計なルールがたくさん入っていたので、仕切り直しということで一旦白紙から再作成する事に。</p>

<p>なんとか基本部分だけ動かす事ができるようになってスコア送信。</p>

<pre><code>2013/10/05 15:45:18 done benchmark
Result:   SUCCESS
RawScore: 14694.2
Fails:    0
Score:    14694.2
</code></pre>

<p>この辺で暫定一位。</p>

<div class='embed tweet'><blockquote class="twitter-tweet"><p><a href="https://twitter.com/search?q=%23isucon&amp;src=hash">#isucon</a> オンライン予選一日目 中間発表二回目です。残り2時間となった16時時点での順位は以下の通り&#10;1. The Revenge of Mr. Frank &amp; Co.&#10;2. hidekiy&#10;3. ( (0) / (0)) ☆祝☆&#10;4. パイの実g&#10;5. 白金動物園&#10;続く</p>&mdash; 941 (@941) <a href="https://twitter.com/941/statuses/386386593696604161">October 5, 2013</a></blockquote>
<script async src="http://ijin.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script></div>


<div class='embed tweet'><blockquote class="twitter-tweet"><p>isuconは外国県人会が暴れてるのかな？</p>&mdash; ばば としあき (@netmarkjp) <a href="https://twitter.com/netmarkjp/statuses/386393051230261248">October 5, 2013</a></blockquote>
<script async src="http://ijin.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script></div>




<br />


<p>しかしその後privateページのキャッシュexpire方法を模索するも、なかなか進まずにタイムリミットが刻々と迫り、他のチームがどんどん上位に食い込んできて若干焦り始める。ひとまず、強制フラッシュするエンドポイントと裏で定期実行するバックグランドタスク（Varnishでいう所のbanlurker相当）を即席で実装したりしてスコアを伸ばすもトップには届かず敢え無く終了。</p>

<p>振り返ってタイムスタンプを見るとギリギリまで粘っていたのが分かりますね。</p>

<pre><code>-rw-r--r-- 1 root root 2322 Oct  5 17:59 /etc/supervisord.conf
</code></pre>

<p>結果、送信できた最終スコアは「<strong>20599.5</strong>」でした（ローカルスコアは22000ぐらいだったけど）。
予選突破（暫定）はできたものの、まだまだ課題山積な感じです。</p>

<h3>良かった点</h3>

<ul>
<li>最初に戦略とスケジュールを策定できた</li>
<li>去年学んだ捨てる勇気を持てたこと</li>
<li>去年よりいろいろ試せたので敗北感は改善</li>
<li>オンライン参戦なので普段使い慣れてる環境で落ち着いて出来た（本戦はアウェー）</li>
<li>今年の密かな目標であるモリスさんに勝ったこと（予選だけど）</li>
</ul>


<h3>反省点</h3>

<ul>
<li>作業ログをもっとしっかり取るべき</li>
<li>バージョン違いはgit mergeせずにbranchをcheckoutするべき</li>
<li>lingrサポート見ればよかった</li>
<li>MySQL5.6のMemcache APIの存在には気づいてたけど、罠とは知らなかった</li>
<li>Newrelic導入に時間かけた割には得られる情報量が薄かった</li>
<li>リバースプロクシが遅れたのは全実装して投入を試みたからでincremental apporachの方が良かった</li>
<li>全員のタスクマネジメントとペアプログラミングをもっとすれば良かった</li>
</ul>


<p>今回も前回同様、非常に楽しめました。
運営の皆様、お疲れ様でした&amp;本戦はさらなる期待をしてます！</p>

<h3>おまけ</h3>

<p>後日行われた各チームの反省会で判明したのがworkloadの存在。どうやらベンチマークツールの並列度を上げる事ができるらしく、そこそこ速くなっているシステムだったらスコアが1.5倍ぐらいは伸びたかもとの事。最終スコアを考えると、30000点台のトップを取れたかもしれないのが無念。。アプローチ自体は総合トップの<a href="https://twitter.com/sechiro">@sechiro</a>さんと<a href="http://sechiro.hatenablog.com/entry/2013/10/07/%23isucon_2013_%E4%BA%88%E9%81%B8%E3%82%92%E3%83%88%E3%83%83%E3%83%97%E9%80%9A%E9%81%8E%E3%81%97%E3%81%A6%E3%81%8D%E3%81%9F%EF%BC%88%E3%81%AF%E3%81%9A%EF%BC%89%E3%80%82">似ていた</a>ので方針は間違ってなかったかと。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS Game Day Tokyo 2013で受賞してきた]]></title>
    <link href="http://ijin.github.io/blog/2013/06/10/aws-game-day-tokyo-2013/"/>
    <updated>2013-06-10T23:33:00+09:00</updated>
    <id>http://ijin.github.io/blog/2013/06/10/aws-game-day-tokyo-2013</id>
    <content type="html"><![CDATA[<p>大統領選挙でオバマ陣営のシステムを堅牢化する為に用いた手法である仮想対戦シミュレーション「<a href="http://gameday2013.doorkeeper.jp/events/3960">AWS Game Day Tokyo</a>」が日本で初めて（世界2番目に）開催されたので参加してきました。</p>

<p>結果、ベスト・ディフェンス賞こと「<strong>Most Awesome Fix!</strong>」賞を受賞しました。</p>

<h2>経緯</h2>

<p>以前、<a href="http://jaws-ug.jp/jawsdays2013/">JAWS DAYS 2013</a>でMiles Wardが講演した「<a href="http://www.publickey1.jp/blog/13/obama_for_america.html">Behind the Scenes of the Presidential Campaign</a>」でチームを攻撃・防御に分けて対戦させ、そこから学んだ事をフィードバックしてシステムをより堅牢にするという「Game Day」を知り、日本でもやりたいねという話になってました。そこで先日の<a href="http://www.awssummittokyo.com/">AWS Summit Tokyo</a>のスピンオフイベントとして、Milesの再来日に合せてADSJ（アマゾンデータサービスジャパン株式会社）さんによって開催される事になりました。チーム戦の大会は<a href="http://ijin.github.io/blog/2012/07/03/tuningathon4/">チューニンガソン</a>や<a href="http://ijin.github.io/blog/2012/11/05/isucon2/">ISUCON</a>以来なので、わくわくしながら速攻で応募をしました。</p>

<h2>概要</h2>

<p>大体、こんな流れです。</p>

<ul>
<li>システムの構築・堅牢化</li>
<li>相手システムの攻撃（この間、自システムも攻撃される）</li>
<li>自システムの修復</li>
<li>評価</li>
</ul>


<p>それぞれ、2〜3人のチームに別れ、計18チームにより対戦。私のチーム名は時事ネタとして今流行りの「<a href="http://jp.techcrunch.com/2013/06/07/20130606report-nsa-collects-data-directly-from-servers-of-google-apple-microsoft-facebook-and-more/">PRISM</a>」としました。
システム概要はnginksさんの<a href="http://d.hatena.ne.jp/nginks/20130608/1370700185">ブログ</a>が非常に分かりやすいです。
要するに画像変換処理バッチクラスターですね。</p>

<h2>構築</h2>

<p>手順書をざっと見ながら画像処理クラスターを構築。相方はシステムの人間ではなかったので、動作確認を手伝ってもらいつつ実質一人でもくもくと作業。s3作成、sqs作成、アプリのインストール・設定・動作確認、AMI化、Cloudwatch設定、AutoScaling設定等を淡々と。構築しながらシステムを把握して行くけど、結構これだけで時間がとられます。なので、じっくりと防衛策は練れなかったのでひとまず、プロセスの自動復旧をしてくれるmonitをインストール・設定し（upstartでやりかけたけどうまく動かなかった）、主要ファイルのchecksumを取って改善検知してメール通知する仕組みを導入。</p>

<h2>攻撃</h2>

<p>AWSキー（Poweruser権限）を奪取したという仮定の元、相手システムに攻撃をしかけるターン。
単純に全部消したり、セキュリティグループの権限を変えたりではあまりにもつまらないので、いろいろ考えます。（無論、システムの全消しは誰でもできる最低の攻撃手法）</p>

<p>まず、状況把握する為にいろいろ動作確認。キューに画像を突っ込んで、ちゃんと処理されるとか。あれ、でも動かない。。
どうやらTokyoで作りかけたけど、結局Virginiaリージョンで仕上げたと運営側から伝えられる。いきなりのタイムロス！</p>

<p>気を取り直して、稼働中のインスタンスに入る方法を思いつく。通常はキー設定されているのでsshでは入れないので仮のインスタンスを起動し旧インスタンスのroot volumeのEBSを強制detachし、仮インスタンスにattachして中身をいじってからre-attachする事に。見た目は同じinstance-idなのに中身だけ違う、一見すると分かりづらいです。そこで旧インスタンスを一旦停止させる為にstopさせると、、terminateされちゃいました。。よくよく調べて見ると、Auto Scalingの設定になっていて、min-sizeの制約によって旧インスタンスが消され、代替インスタンスが自動的に起動するようになってました。</p>

<p>どうやら構築が間に合わなかったチーム用に運営側が用意した自動構築をしてくれる虎の子のCloudformationを使った模様。
そこで、相手チームのスキルレベルがそれ程高くないと判断し、Auto Scalingの元AMIを置き換える事に。
新たなlaunch configを作成し、既存のscaling groupと同盟のものを作成。</p>

<p>次にs3への攻撃。bucket名はglobalなnamespaceなので、こいつを削除して同名のを別AWSアカウントで作れば乗っ取りが可能。。
のはずが、削除してから一定時間経過しないと作成不可だったので1字違いのbucketを作成しておく。</p>

<p>最後にs3のbucket一覧を取得して、常に空のディレクトリと同期し続ける攻撃を思いつき、実装を始める。システムはs3に出力するのでそこを継続的に空にする攻撃です。しかし、実装を初めて動作確認の途中で時間切れになりシステムに埋め込めなくて断念。もうちょっと時間が欲しかったです</p>

<h2>修復</h2>

<p>次は自システムが受けた攻撃を修復するターン。</p>

<p>いろいろ余計なインスタンスが起動していたが、まずやったのがAMI番号の確認。（これが無事であればOSに侵入されていようがAMIをベースに全体の再構築が速やかにできるので）
幸い、控えていたのと一致していたので他のインスタンスを全て停止。一応monitのアラートメールが飛んでいなかったので、インスタンスに対しての操作は限定されているのだろうとは踏んでましたが。</p>

<p>AMIが無事なら次はAuto Scalingの確認。ざっと見た感じだと、lauch configは操作されておらず、scaling groupのmin-sizeが0に変更されている模様。他の変更点は確認が面倒だったので、一旦全部削除してさくっと再作成。後で聞いた話だと、Auto Scalingのrecurring schedule設定で1分起きに0台にする設定をしていたらしいが、消された時点で攻撃は無効化。</p>

<p>次にSQS。消して再作成すればてっとり早いけど、相手チームがキューに投入した画像を最終的に表示させないといけないルールだと誤解していて、その復旧に務める。新しく作ったSQSと比較するとパラメータ（Default Visibility Timeout, Retention Period, Message Size等）が異常な値に変更されていると分かり、通常の値へ戻す。</p>

<p>この時点でアプリとSQSの通信を確認するも疎通できない事を把握。pingが通らない事からSecurity Group, Routing Table, Network ACLが変更されていないかを確認。どうやらSecurity GroupのIn/Outルールが削除されている単純な攻撃だと判明し、なんなく再設定。</p>

<p>キュー内のメッセージが1コ処理されるのを確認し、SQS周りは対応済みかと思ったけど残り2コのメッセージがいくら待てども処理されず、若干悩む。
ログを見たり、メッセージの中身を覗くとと「<strong>&#8211;max-redirect=99999999</strong> 」が目に留まる。どうやら変換する画像をダウンロードする部分で無限ループさせている模様。メッセージを削除し、そのパラメータを除外したものを流してちゃんとキューが処理される事を確認。</p>

<p>最後にs3周りで怪しい設定がないかを調査して、一通りの動作確認をして復旧完了。</p>

<h2>振り返り</h2>

<p>お互いに対戦したチームと顔合わせをし、攻撃や復旧の手の内を明かします。全チームの行動記録を集約して運営側で審査を行われ各賞が授与される中、我がPRISMはSQS内の無限ループを検知・修復したのが評価されて最も優れた修復を行った「<strong>Most Awesome Fix!賞</strong>」を頂きました。後で他のチームに聞いた所、monitのような検知・通知の仕組みを導入した所はなさそうだったので、それも評価ポイントだったかも知れません。</p>

<p>賞の内容としては、ワンタイムトークンを生成するハードウェアMFAデバイスとAWSのクーポンコードでした。ありがとうございます。</p>

<p>最後にMilesが壊れても戻せるようにあるべき状態の定義と常に比較して自動的に自己治癒するのが最高のシステムと言ってました。AWSの状態を保存するにはCloudformerでCloudformationテンプレート化すれば便利で楽だけど、chefみたいにIdempotency（冪等性）を継続的に保証する仕組みをそのレイヤーで組むのはなかなか大変ですね。（個々のサーバ単位は可能だとしても）</p>

<h2>感想</h2>

<p>「<em>チューニンガソン</em>」や私が他にお手伝いをしている「<em>トラブル☆しゅーたーず</em>」とも一味違って、非常に楽しめました。</p>

<p>以下、思った事をいくつか。</p>

<ul>
<li>構築に時間が取られたのでなるべく出来合いのシステムがあった方が防衛策に専念できそう</li>
<li>攻撃可能な時間が思ったより短かったのでもうちょっと長めで</li>
<li>Default-VPCとEC2-Classicでは挙動が違うのでアカウントタイプは統一した方が良い</li>
<li>ターン性ではなく、攻撃と修復・防御のリアルタイム性を試すとか</li>
<li>あるべき正しい状態を把握する為のツールがあると他のタスクに集中できるかも</li>
</ul>


<p>次に開催される時も参加したいですね。もしくは運営側のお手伝いでも！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Non-RDSなオレオレMulti-AZ MySQL Replication]]></title>
    <link href="http://ijin.github.io/blog/2013/05/21/custom-non-rds-multi-az-mysql-replication/"/>
    <updated>2013-05-21T18:03:00+09:00</updated>
    <id>http://ijin.github.io/blog/2013/05/21/custom-non-rds-multi-az-mysql-replication</id>
    <content type="html"><![CDATA[<p>先日（5/17/13）<a href="http://www.zusaar.com/event/668006">cloudpack night #6 - Re:Generate -</a>に参加してきました。</p>

<p>当日の様子はcloudpack<a href="https://twitter.com/yoshidashingo">吉田</a>さんの以下のレポートで。<br />
<a href="http://d.hatena.ne.jp/yoshidashingo/20130518/1368853720">cloudpack Night #6 - re:Generate - を開催しました</a></p>

<p>私はDJを少々とLTで参加させて頂いたのでその内容になります。</p>

<h2>オレオレMulti-AZのススメ</h2>

<p>AWS上でMySQLを使う場合、RDSはてっとり早くて良いんですが、たまにもうちょっと柔軟性が欲しい時があります。
例えば別のストーレジエンジンやディストリビューションを使ったり（Percona Server, MariaDB, TokuDB, Mronnga等）、RDSでは使えないインスタンスファミリー（hi1, m3, c1系）を使ったり、OSレベルでのチューニングができたり、スレーブのバッファプールを予め温めておいたり。等々。</p>

<p>しかし、RDSにはAvailability Zone (AZ)をまたいでフェールオーバーするMulti-AZ機能があり、AWSで設計するにはAZ障害を考慮した方が推奨されます。</p>

<p>そこで、MHAとVPCを組み合わせて柔軟性をもったMulti-AZ環境を実現します。
（ちなみに私の場合はhi1.4xlargeでPercona Serverを冗長化をする必要があったから）</p>

<h3>MHA</h3>

<p><a href="https://code.google.com/p/mysql-master-ha/">MHA</a>とはFacebookの<a href="http://yoshinorimatsunobu.blogspot.jp/">松信</a>さんがDeNA時代に作ったMySQLの自動フェールオーバーしてくれうナイスなツールです。Master障害時にbinlog同期とSlaveの昇格を全自動でやってくれます。昇格時にはカタログデータベースに更新をかけて新DB構成のIPの情報を更新するか、Virtual IPを切り替えるかをする必要（この担当部分はmaster_ip_failover_scriptで対応）があるけど、今回は後者的なアプローチになります。</p>

<h3>VPC</h3>

<p>ENIを使えばVirtual IP的な使い方はできるけどAZは超えられないので、source/dest. checkを無効化した上でrouting tableによって擬似Virtual IPを実現する<a href="http://aws.clouddesignpattern.org/index.php/CDP:Routing-Based_HA%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3">Routing-Based HAパターン</a>（CDP 2.0候補）を使います。ADSJ<a href="https://twitter.com/c9katayama">片山</a>さんの<a href="http://d.hatena.ne.jp/c9katayama/20111225/1324837509">エントリ</a>が発端ですね。</p>

<h2>Demo</h2>

<p>以下、LT時に見せたデモ動画</p>

<div class="embed-video-container"><iframe src="http://www.youtube.com/embed/tovb29K6ddc "></iframe></div>


<p>擬似Virtual IPに対してそれぞれread/writeを行いつつMasterのプロセスをkillすると、通信できなくエラーが出続けるが20秒以内に自動フェールオーバーが完了しread/write共に再開します。pingも平均0.5msから2.0msに変わった事からAZが移った事が確認できます。</p>

<p>RDSのMulti-AZの場合、フェールオーバーには3-6分かかるので、かなり短時間で復旧が可能です。
また、RDSはCNAME切替によるDNSベースに対して、MHA+VPC構成の場合はIP指定することができます。そうするとアプリからのresolveが不要になり、去年起こった内部DNSが引けない障害が起きても問題ありません。</p>

<h2>注意点</h2>

<ul>
<li>RDSな自動バックアップがない

<ul>
<li>Xtrabackupとbinlogの定期s3保存で対応</li>
</ul>
</li>
<li>Point in Timeリカバリー

<ul>
<li>Chef等で自動化しましょう</li>
</ul>
</li>
<li>Read Replicaの作成

<ul>
<li>Chefで頑張りましょう</li>
</ul>
</li>
<li>学習曲線

<ul>
<li>勉強しましょう</li>
</ul>
</li>
<li>API backplaneがSPoF

<ul>
<li>AWSに祈りましょう</li>
</ul>
</li>
</ul>


<p>特に一番の懸念点は最後のAPI backplane。AWSの今までの大規模障害状況を見ていると、皆が同時に復旧をしようとしAPIへのリクエストが大量に集中してそこがボトルネックになり、リソースの操作不能に陥るという悲惨な事象が何回かありました。まあ、その場合はRDSでも同じような気はしますが、ここは当時の障害を経験にキャパシティが増加されている事を信じておくしかありませんね。。</p>

<h2>おわりに</h2>

<p>とまあ、こんなLTをしたわけですが、この後に続いたCookpadの<a href="https://twitter.com/sgwr_dts">菅原</a>さんの<a href="http://www.slideshare.net/winebarrel/ec2keepalivedlvsdsr">LT</a>の方が盛り上がって自分のは余興に終わってしまいました。</p>

<p>あ、ついでにその日はcpniteの資料作成やDJの選曲であんまり寝てなかったにもかかわらず、無事AWSソリューションアーキテクト（Associate）の認定試験に受かりました！</p>

<p><img src="https://lh5.googleusercontent.com/-d_HVsb6DgBc/UZs39XB1KGI/AAAAAAAAAuw/ntgPp33hcOI/w294-h120-no/Solutions-Architect-Associate.png"></p>

<h2>LTスライド</h2>

<div class="embed-ss-container"><iframe src="http://www.slideshare.net/slideshow/embed_code/21341276 "></iframe></div>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[リージョン間高速データ転送]]></title>
    <link href="http://ijin.github.io/blog/2013/04/03/accelerating-cross-region-data-transfer/"/>
    <updated>2013-04-03T09:05:00+09:00</updated>
    <id>http://ijin.github.io/blog/2013/04/03/accelerating-cross-region-data-transfer</id>
    <content type="html"><![CDATA[<p>先日の<a href="http://jaws-ug.jp/jawsdays2013/">JAWS DAYS</a>のセッション<a href="http://www.publickey1.jp/blog/13/obama_for_america.html">「Behind the scenes of Presidential Campaign」</a>でリージョン間のデータ転送を高速化するツールとして<a href="http://tsunami-udp.sourceforge.net">tsunami udp</a>と<a href="http://www.cloudopt.com">cloudopt</a>を使った話が出てたので試してみました。</p>

<p>通常、遠距離のサーバはRTTが大きくなるのでスループットが下がり、巨大なデータ転送には苦労しますが、なんと27TBを9時間で転送したとのこと！実際はマシンを並列にして同時転送したらしいので1台での実験です。</p>

<h3>前提</h3>

<ul>
<li>東京(server1) -> アメリカ西海岸(server2)</li>
<li>RTT: 120msぐらい</li>
<li>EC2 instance type: m1.large</li>
<li>OS: Ubuntu 12.04</li>
</ul>


<h3>ベーステスト</h3>

<p>10Gファイルの作成</p>

<pre><code>server1$ mkdir _tmp; cd _tmp
server1$ dd if=/dev/zero of=10G count=1 bs=1 seek=10G
</code></pre>

<p>転送</p>

<pre><code>server1$ scp -rp 10G server2:
10G     100%   10GB  11.1MB/s   15:22
</code></pre>

<p>だいたい、90Mbpsぐらい。</p>

<h3>Tsunami UDP</h3>

<p>インストールは両サーバで</p>

<pre><code>sudo apt-get install make gcc autoconf
wget http://downloads.sourceforge.net/project/tsunami-udp/tsunami-udp/tsunami-v1.1-cvsbuild42/tsunami-v1.1-cvsbuild42.tar.gz
tar xvfz tsunami-v1.1-cvsbuild42.tar.gz
cd tsunami-udp-v11-b42
make
sudo make install
</code></pre>

<p>tsunami udpはfetch型の作りなので、送信側のサーバ（server1）で対象ファイルのあるディレクトリに移動してサービス起動</p>

<pre><code>server1$ cd _tmp
server1$ tsunamid *
</code></pre>

<p>で、クライアント(server2)からファイルをfetch</p>

<pre><code>server2$ tsunami connect ec2-xx-x-xx-x-x get 10G quit
</code></pre>

<p>本当はftp-likeは対話型クライアントだけど、ワンライナーでも可能。  <br/>
転送が完了するとクライアント側で情報が出力されます。</p>

<pre><code>Transfer complete. Flushing to disk and signaling server to stop...
!!!!
PC performance figure : 224947 packets dropped (if high this indicates receiving PC overload)
Transfer duration     : 419.32 seconds
Total packet data     : 183339.18 Mbit
Goodput data          : 181958.17 Mbit
File data             : 81920.00 Mbit
Throughput            : 437.23 Mbps
Goodput w/ restarts   : 433.93 Mbps
Final file rate       : 195.36 Mbps
Transfer mode         : lossless
</code></pre>

<p>おお、確かに速い！スループットもセッションでも言ってた476Mbpsに近いし。</p>

<p>サーバ側では</p>

<pre><code>Server 1 transferred 10737418241 bytes in 419.33 seconds (195.4 Mbps)
</code></pre>

<p>ファイル自身の実際の転送レートはFinal file rateである<strong>195.4Mbps</strong>。
多分、パラメータチューニングやより速いディスクを使うとスループットはさらに向上すると予想。</p>

<h3>cloudopt</h3>

<p>次は圧縮、TCP最適化、data deduplication等、様々な技術を用いてWANを高速化する<a href="http://www.cloudopt.com">cloudopt</a>の実験。こちらは有料で、ソフトウェアをインストールしてライセンスを購入（15日間のお試しあり）して使う方法とAmazon Marketplaceでセットアップ済みの課金型AMIを起動する方法がとれます。今回はてっとり早く後者で。</p>

<p>AMIはMarketplaceでCloudoptを検索し、各リージョンで1台づつ起動。</p>

<p><img src="https://lh5.googleusercontent.com/-7PWSb_8ClIo/UVuL_bzBeAI/AAAAAAAAAtc/8Lw_bDnaPVE/s665/cloudopt_marketplace_+2013-03-30+at+2.08.07+PM.png"></p>

<p>Ubuntuベースなのが良いですね。</p>

<p>まずscpから呼べるcloudcopyを使っての転送</p>

<pre><code>server1$ scp -rp -S cloudcopy 10G server2:
10G                 100%   10GB  41.5MB/s   04:07     
CloudCopy transferred 17.22 MB, saving 99.8% of bandwidth by sending 9.983 fewer GB 
</code></pre>

<p>お、速い。しかしよく見てみると17.22MBしか転送されてません。どうやらファイル自体が全てゼロ埋めなので圧縮とdeduplicationが最大限効いているみたい。</p>

<p>では、今度は実際のDBのバックアップで転送実験（容量14GB）</p>

<pre><code>server1$ scp -rp -S cloudcopy dbbackup.tar server2:
dbbackup.tar                100%   14GB  11.2MB/s   20:42   
CloudCopy transferred 6.041 GB, saving 55.5% of bandwidth by sending 7.528 fewer GB
</code></pre>

<p>スループットはほぼ一緒だけど、転送容量が削減できてます。</p>

<p>一度転送したファイルはbyte cacheされるので、次に転送する時は差分だけ送るので高速化されます。</p>

<pre><code>server2$ rm dbbackup.tar 

server1$ tar rvf dbbackup.tar file
server1$ scp -rp -S cloudcopy dbbackup.tar server2:
dbbackup.tar                100%   14GB  20.0MB/s   11:35  
CloudCopy transferred 59.57 MB, saving 99.6% of bandwidth by sending 13.511 fewer GB
</code></pre>

<h3>リージョン間レプリケーション</h3>

<p>次はMySQLのレプリケーション実験。</p>

<p>各インスタンスでのピア設定</p>

<pre><code>server1$ sudo cloudconfig peer_add server2 server2_local_ip(10.x.x.x)
server2$ sudo cloudconfig peer_add server1 server1_local_ip(10.x.x.x)
</code></pre>

<p>これでcloudoptを使ったサーバ間のトンネルが確立されます。MySQLのレプリケーションはprivate ipでの設定が必要。
一通りレプリケーションが出来き、スレーブIOを停止した状態でマスターにしばらく書き込んだのちに再開すると、binlogが転送されるのでcloudstatsコマンドで統計が見れます。</p>

<pre><code>Component - cloudoptimizer
------------------------------------------------------------
           Number of connections:                    1
              Original data size:            111.96 MB
           Transferred data size:             52.44 MB

            Bandwidth Saving:             59.51 MB (53.2%)
</code></pre>

<p><strong>53.2%</strong>の転送容量削減！</p>

<h3>結論</h3>

<p>以上を組み合わせれば、新たなCDP候補である「<strong>リージョン間レプリケーションパターン (Cross-Region Replication Pattern)</strong>」が実現できます。</p>

<ul>
<li>巨大データをリージョン間で転送するにはtsunami udpが有効そう</li>
<li>リージョン間での差分バックアップやレプリケーション向けにはcloudoptで高速化</li>
<li>普通のHTTP通信とかも使えるかも</li>
<li>s3へのアップロード高速化も対応しているのでいずれ試してみたい</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS SSD (hi1.4xlarge) vs Fusion-IOでのMySQLベンチマーク]]></title>
    <link href="http://ijin.github.io/blog/2013/02/22/mysql-benchmarks-on-aws-ssd-vs-fusion-io/"/>
    <updated>2013-02-22T17:32:00+09:00</updated>
    <id>http://ijin.github.io/blog/2013/02/22/mysql-benchmarks-on-aws-ssd-vs-fusion-io</id>
    <content type="html"><![CDATA[<p>（※ <a href="#add">追記</a>しました - 5/19/13）</p>

<p>巷ではMySQL 5.6 GAが出て騒がしいですが、ちょっと前に5.5系でAWSのSSDインスタンス（hi1.4xlarge）に載せ替える案件があったので、その時に取ったベンチマークを公表します。以前Fusion-IO (ioDrive Duo)でも同じようにやったので、比較になれば。</p>

<h2>経緯</h2>

<ul>
<li>あるウェブサービスのDBサイズが巨大でm2.4xlargeでも辛くなってきている</li>
<li>アクセスパターンによりパーティショニングが効かない</li>
<li>シャーディングをするにはアプリ改修が大変</li>
<li>数週間後に急激なアクセスが予想され、時間的余裕がない！</li>
<li>データサイズの急激な増加によりbuffer poolから溢れ、ディスクアクセスのさらなる発生が懸念</li>
</ul>


<p>というわけで、時間がないのでSSDへの移行を検討し、ベンチマークを取りました。</p>

<h2>ベンチマーク</h2>

<p>buffer poolが徐々に足りなくなった場合のディスクアクセスの発生をシミュレート</p>

<ul>
<li>sysbenchのoltpモード</li>
<li>データサイズは12G（5000万件）</li>
<li>readonly</li>
<li>uniform（フルスキャン）</li>
</ul>


<p>主要my.cnfパラメータ</p>

<pre><code> sync_binlog = 0
 innodb_buffer_pool_size = XXG
 innodb_flush_method = O_DIRECT
 innodb_flush_log_at_trx_commit = 1
 innodb_file_per_table
 innodb_io_capacity = 2000
</code></pre>

<p>コマンド</p>

<pre><code> time sysbench --test=oltp --oltp-table-size=50000000 --db-driver=mysql --mysql-user=root prepare                                                                                                         
 time sysbench --test=oltp --oltp-table-size=50000000 --db-driver=mysql --mysql-user=root --num-threads=16 --max-requests=0 --max-time=180 --init-rng=on --oltp-read-only=on --oltp-dist-type=uniform 2&gt;&amp;1 run                                                                                                         
</code></pre>

<h2>結果</h2>

<p>トランザクション推移</p>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&oid=2&zx=ii4lryrf8pz8"></p>

<p>レスポンスタイム推移</p>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&oid=3&zx=c2drap7b5561"></p>

<p>Fusion-IOと比べて半分ぐらい。ioDrvie Duoの公称IOPSが250,000+ IOPSでhi1.4xlargeが120,000 IOPSなので、まあ合致しますね。</p>

<p>まだ整理されてないベンチマーク結果があるので、後ほど追記しようと思います。</p>

<h2>追記 <a name="add">&nbsp;</a></h2>

<p>先日（5/17/13）の<a href="http://www.zusaar.com/event/668006">cloudpack night #6 - Re:Generate -</a>でADSJの荒木さんの発表でMySQLのパフォーマンスの話があったので<a href="https://code.launchpad.net/~percona-dev/perconatools/tpcc-mysql">tpcc-mysql</a>でベンチマークを取った資料を思い出し追記しました。</p>

<ul>
<li>500 warehouses (50GBぐらい)</li>
<li>24GB Buffer pool</li>
<li>16スレッド</li>
<li>1時間実行</li>
</ul>


<p>コマンド（ロードはかなり時間がかかるので注意）</p>

<pre><code> tpcc_load localhost tpcc root "" 500
 tpcc_start -d tpcc -u root -p "" -w 500 -c 16 -r 300 -l 3600
</code></pre>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&oid=4&zx=y6kwyezb1qth"></p>

<p>hi1.4xlargeの方が安定するまで少し時間がかかってます。<br />
以下、安定化しだした900sあたりからの数値です。</p>

<table>
<thead>
<tr>
<th></th>
<th>Fusion-IO</th>
<th>hi1.4xlarge</th>
</tr>
</thead>
<tbody>
<tr>
<td>total</td>
<td>2288758</td>
<td>1444417</td>
</tr>
<tr>
<td>avg</td>
<td>8445.6</td>
<td>5330.0</td>
</tr>
<tr>
<td>stddev</td>
<td>245.7</td>
<td>132.8</td>
</tr>
</tbody>
</table>


<p>Fusion-IOに比べて6割ってところでしょうか。
今回はSSDのephemeral disk1本で試したので、RAID0にするともうちょっと違ってくると思います。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[非ELBなAutoscalingによる自動復旧]]></title>
    <link href="http://ijin.github.io/blog/2013/02/08/self-healing-with-non-elb-autoscaling/"/>
    <updated>2013-02-08T09:29:00+09:00</updated>
    <id>http://ijin.github.io/blog/2013/02/08/self-healing-with-non-elb-autoscaling</id>
    <content type="html"><![CDATA[<p>バッチ処理等、サーバの冗長化が難しく仕方なく1台で動かさざるを得ない場合があります。でも可用性は確保したい。また、Pacemakerやkeepalived等できなくはないが、お金もあんまりかけられない場合もあります。
そんな時にAWS上でよく使うのがAutoscalingによる1台構成です。最低台数・最大台数共に「1」に設定しておけばEC2インスタンスが壊れても自動的に新しいのにリプレースされて復旧されます。</p>

<p>しかし、Autoscalingのhealth-check-typeを「EC2」にした場合、インスタンスの起動状態（running, stopped）しか見てくれないので、今までこの構成を実現するのにELBによる死活監視が必要でした。インスタンスがHTTPサーバじゃない場合、ちょっとムダです。</p>

<p>ところが、ちょっと前にAutoscalingがインスタンスの健康状態をチェックするEC2 status checkに<a href="http://aws.amazon.com/about-aws/whats-new/2012/12/14/auto-scaling-now-uses-amazon-ec2-status-checks/">対応</a>し、ELBが不要になったはずなので試してみました。</p>

<p>今回は各種AWSサービスに対応した統合されたPython版の<a href="http://aws.amazon.com/cli/">AWS CLI</a>ツールを使います。</p>

<h3>セットアップ</h3>

<p>まずは、ツールのインストール</p>

<pre><code>sudo apt-get install python-pip
sudo pip install awscli
complete -C aws_completer aws
</code></pre>

<h3>Autoscaling設定</h3>

<p>Launch Configの設定</p>

<pre><code>aws autoscaling create-launch-configuration --image-id ami-4a12aa4b \
--launch-configuration-name test-lc --instance-type t1.micro --key-name ijin-tokyo \
--security-groups test --iam-instance-profile test_iam

{
    "ResponseMetadata": {
        "RequestId": "c0e66974-7103-11e2-9780-a53199bac60e"
    }
}
</code></pre>

<p>Scaling Groupの設定</p>

<pre><code>aws autoscaling create-auto-scaling-group --auto-scaling-group-name test-sg \
--launch-configuration-name test-lc --min-size 1 --max-size 1 \
--health-check-grace-period 180 --tags '{"key":"Name", "value":"as-test"}' \
'{"key":"Use Case", "value":"test"}' --availability-zones ap-northeast-1a --health-check-type "EC2"

{
    "ResponseMetadata": {
        "RequestId": "e3808ef3-7103-11e2-9780-a53199bac60e"
    }
}
</code></pre>

<p>通知</p>

<pre><code>aws autoscaling put-notification-configuration --auto-scaling-group-name test-sg \
--topic-arn arn:aws:sns:ap-northeast-1:521026608000:test \
--notification-types autoscaling:EC2_INSTANCE_LAUNCH autoscaling:EC2_INSTANCE_TERMINATE \
autoscaling:EC2_INSTANCE_LAUNCH_ERROR autoscaling:EC2_INSTANCE_TERMINATE_ERROR

{
    "ResponseMetadata": {
        "RequestId": "f68359be-7103-11e2-9a1a-5f77b12b596e"
    }
}
</code></pre>

<p>レスポンスがjsonなのが良いですね。
また、<a href="http://aws.amazon.com/developertools/2535">Auto Scaling Command Line Tool</a>と違って、tagでスペースが使えるようになったのが素晴らしい！</p>

<p>以上の設定でAutoscalingによってインスタンスが1台立ち上がります。</p>

<h3>自動復旧</h3>

<p>最後にインスタンス不調（status check failure）をシミュレートする為にインスタンス内からネットワークを落とします。</p>

<pre><code>ubuntu@ip-10-128-25-25:~$ sudo ifdown eth0
</code></pre>

<p>これでstatus checkがfailし、Autoscalingが自動的に新しいインスタンスと取り替えてくれるはず！</p>

<p><img src="https://lh6.googleusercontent.com/-9FpM6ywjg84/URN6q29FD-I/AAAAAAAAAsw/paP8HBnisPE/s161/aws_status_check_2013-02-07+18.06.20.png"></p>

<p><img src="https://lh5.googleusercontent.com/-5avxLvp5RH8/URN6n-ihwiI/AAAAAAAAAso/zOHxli4vM8o/s702/aws_instance_check_2013-02-07+18.09.19.png"></p>

<p>と、期待して待っていたらなかなかアラートメールが来ません。。</p>

<p>設定間違ったかなーっていろいろ見直していたら20分経った頃にやっと到着。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Service: AWS Auto Scaling
</span><span class='line'>Time: 2013-02-07T09:17:42.304Z
</span><span class='line'>RequestId: f395660b-4847-4415-ad33-f8cc5091bdb3
</span><span class='line'>Event: autoscaling:EC2_INSTANCE_TERMINATE
</span><span class='line'>AccountId: 521026608000
</span><span class='line'>AutoScalingGroupName: test-sg
</span><span class='line'>AutoScalingGroupARN: arn:aws:autoscaling:ap-northeast-1:521026608000:autoScalingGroup:a036877b-dab7-4e5d-a6e1-1d3424b20d14:autoScalingGroupName/test-sg
</span><span class='line'>ActivityId: f395660b-4837-4415-ad33-f8cc5071bdb3
</span><span class='line'>Description: Terminating EC2 instance: i-15fadd17
</span><span class='line'>Cause: At 2013-02-07T09:16:57Z an instance was taken out of service in response to a system health-check.
</span><span class='line'>StartTime: 2013-02-07T09:16:57.389Z
</span><span class='line'>EndTime: 2013-02-07T09:17:42.304Z
</span><span class='line'>StatusCode: InProgress
</span><span class='line'>StatusMessage:
</span><span class='line'>Progress: 50
</span><span class='line'>EC2InstanceId: i-15fadd17
</span><span class='line'>Details: {}</span></code></pre></td></tr></table></div></figure>


<p>うーん。動く事は動いたけど、ちょっと時間がかかるなぁ。</p>

<p>この後何回か試してみたけど、Autoscaling発動までどれも20分ぐらいかかりました。</p>

<h3>結論</h3>

<p>20分程サーバダウンが許容できるようなゆるめの条件に限定した場合には非ELBでも使えるかな。まあ、それでも適用する場面は多々あるとは思いますが。（早める方法あるのかなー。）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Auto Scalingの設定とデプロイ方法]]></title>
    <link href="http://ijin.github.io/blog/2012/12/03/cdp/"/>
    <updated>2012-12-03T01:34:00+09:00</updated>
    <id>http://ijin.github.io/blog/2012/12/03/cdp</id>
    <content type="html"><![CDATA[<p><a href="http://www.zusaar.com/event/451061">CDP Advent Calendar 2012</a>に登録しました。ここ1年ちょいで使い慣れてきたパターンがあり、作った当時はクラウドデザインパターンはなかったのですが、<a href="http://aws.clouddesignpattern.org/index.php/CDP:Clone_Server%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3">Clone Server</a>と<a href="http://aws.clouddesignpattern.org/index.php/CDP:Scale_Out%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3">Scale Out</a>パターンの組合せに当てはまると思うので紹介します。ちなみにアプリはRails。</p>

<p>常にデプロイして更新し続けるシステムを手動、あるいは自動スケールアウトする時に便利な手法だったりします。</p>

<p>図にするとこんな感じですかね。</p>

<p><img src="https://lh4.googleusercontent.com/-eDSfSiz0XhU/ULyOlKj_JKI/AAAAAAAAArw/QzSfVJeeUfw/w727-h490-p-k/CDP_Clone%252BScale_Out.png"></p>

<h2>Auto Scaling設定</h2>

<h3>Launch Config</h3>

<p>まず、ベースとなるAMIの起動インスタンスサイズやセキュリティーグループを定義したLaunch Configを設定。</p>

<pre><code>as-create-launch-config cdp-lc --image-id ami-22a51d23 --instance-type m1.small \
--group cdp_web, cdp_admin
</code></pre>

<h3>Scaling Group</h3>

<p>次に、Scaling Groupで適用するAvailability Zone、インスタンス数の最小・最大閾値設定、紐尽くELB、死活監視方法・開始待ち時間、タグ等を定義。（ec2addtagではスペース入りのキーを設定できるのにas-create-auto-scaling-groupではできないので注意。早く直して。。）</p>

<pre><code>as-create-auto-scaling-group cdp-sg --launch-configuration cdp-lc \
--availability-zones ap-northeast-1a  --min-size 1 --max-size 9 --load-balancers cdp \
--health-check-type ELB --grace-period 300 --tag "k=Name, v=web.cdp" --tag "k=Use_Case, v=Test"
</code></pre>

<h3>Scaling Out Policy</h3>

<p>Scaling Outのポリシー設定。以下の例ではインスタンス1台を追加し、次のスケールアクションまでは5分間待機。ポリシーのARN (Amazon Resource Name)が出力されるので控えます。</p>

<pre><code>as-put-scaling-policy CDPOut --auto-scaling-group cdp-sg --adjustment 1 --type ChangeInCapacity \
--cooldown 300

&gt; arn:aws:autoscaling:ap-northeast-1:494850320039:scalingPolicy:d0d4dcf1-fb44-428a-a19c-38946633acf5:autoScalingGroupName/cdp-sg:policyName/CDPOut
</code></pre>

<h3>Cloudwatchトリガー（Scaling Out）</h3>

<p>Cloudwatchで閾値を超えたらスケールアウトするように設定。以下の例では対象Scaling Group配下のインスタンス達のCPUが1分間で平均75%以上で推移した場合、先ほど設定したScaling OutポリシーがARN経由で実行されます。また、閾値を超過あるいは下回った場合にSNS経由でアラートメールを飛ばします。</p>

<pre><code>mon-put-metric-alarm  CDPHighCPUAlarm --comparison-operator GreaterThanThreshold \
--evaluation-periods 1 --metric-name CPUUtilization --namespace "AWS/EC2" --period 60 \
--statistic Average --threshold 75 --dimensions "AutoScalingGroupName=cdp-sg" \
--ok-actions arn:aws:sns:ap-northeast-1:494850320039:cdp-alert --alarm-actions \
arn:aws:sns:ap-northeast-1:494850320039:cdp-alert, arn:aws:autoscaling:ap-northeast-1:494850320039:scalingPolicy:d0d4dcf1-fb44-428a-a19c-38946633acf5:autoScalingGroupName/cdp-sg:policyName/CDPOut
</code></pre>

<h3>Scaling Down Policy</h3>

<p>同じくScaling Downのポリシー設定。インスタンスが起動すると最低1時間は課金されるので頻繁に伸縮するともったいないのでスケールダウンアクションはゆっくりやるのがポイントです。</p>

<pre><code>as-put-scaling-policy CDPDown --auto-scaling-group cdp-sg --adjustment=-1 \
--type ChangeInCapacity --cooldown 1500

&gt; arn:aws:autoscaling:ap-northeast-1:494850320039:scalingPolicy:de53fbd5-130c-46a8-bf47-25e29f7d358e:autoScalingGroupName/cdp-sg:policyName/CDPDown
</code></pre>

<h3>Cloudwatchトリガー（Scaling Down）</h3>

<p>スケールダウンのトリガーは平均CPUが35%を25分間下回った場合に実行されます。この閾値周辺のアラートメールはいらないので設定してません。</p>

<pre><code>mon-put-metric-alarm CDPLowCPUAlarm --comparison-operator LessThanThreshold \
--evaluation-periods 1 --metric-name CPUUtilization --namespace "AWS/EC2" --period 1500 \
--statistic Average --threshold 35 --dimensions "AutoScalingGroupName=cdp-sg" --alarm-actions \
arn:aws:autoscaling:ap-northeast-1:494850320039:scalingPolicy:de53fbd5-130c-46a8-bf47-25e29f7d358e:autoScalingGroupName/cdp-sg:policyName/CDPDown
</code></pre>

<h3>通知</h3>

<p>最後にスケールアクション時に通知が飛ぶように設定。例えば、あるインスタンスが不調により反応がなく、ポリシーによってリプレースされる場合に飛んだりします。</p>

<pre><code>as-put-notification-configuration cdp-sg -t arn:aws:sns:ap-northeast-1:494850320039:cdp-alert \
-n autoscaling:EC2_INSTANCE_LAUNCH, autoscaling:EC2_INSTANCE_TERMINATE, \
autoscaling:EC2_INSTANCE_LAUNCH_ERROR, autoscaling:EC2_INSTANCE_TERMINATE_ERROR
</code></pre>

<h2>APP側の設定</h2>

<h3>ソースコード同期</h3>

<p>同期には起動時にupstart経由でrsyncを叩いて管理サーバから最新ソースを取ってきてサービスを再起動します。転送時の圧縮モードはarcfourが一番スループットが出たのでそれに。</p>

<div><script src='https://gist.github.com/4191986.js?file='></script>
<noscript><pre><code># sync source code
description &quot;Update source code&quot;

start on filesystem
stop on shutdown

task
console output

script
time sudo -u deploy rsync -av --delete -e 'ssh -c arcfour -i /home/deploy/.ssh/id_dsa -o StrictHostKeyChecking=no' deploy@admin.cdp:/usr/local/apps/project/ /usr/local/rails_apps/project &gt;&gt; /mnt/update.log 2&gt;&amp;1
/etc/init.d/unicorn upgrade &gt;&gt; /mnt/update.log 2&gt;&amp;1
/etc/init.d/nginx restart &gt;&gt; /mnt/update.log 2&gt;&amp;1
end script
</code></pre></noscript></div>


<h2>Capistrano</h2>

<p>デプロイはマスターサーバ（管理兼）上でCapistranoを実行し、ELB配下の生きているインスタンスに対して更新をかけます。deploy.rbに追加する記述は以下の通り。昔はec2 api toolsを直接呼んでパースしたりしてコードが長かったのですが、今は<a href="http://aws.amazon.com/sdkforruby/">AWS SDK for Ruby</a>があり、<a href="http://aws.amazon.com/iam/">IAM role</a>でinstance profileを設定すればわずか数行でできてしまいます！</p>

<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="nb">require</span> <span class="s1">&#39;aws-sdk&#39;</span>
</span><span class='line'><span class="no">AWS</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="ss">:ec2_endpoint</span> <span class="o">=&gt;</span> <span class="s1">&#39;ec2.ap-northeast-1.amazonaws.com&#39;</span><span class="p">,</span> <span class="ss">:elb_endpoint</span> <span class="o">=&gt;</span> <span class="s1">&#39;elasticloadbalancing.ap-northeast-1.amazonaws.com&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">instances</span> <span class="o">=</span> <span class="no">AWS</span><span class="o">::</span><span class="no">ELB</span><span class="o">.</span><span class="n">new</span><span class="o">.</span><span class="n">load_balancers</span><span class="o">[</span><span class="s1">&#39;cdp&#39;</span><span class="o">].</span><span class="n">instances</span><span class="o">.</span><span class="n">select</span><span class="p">{</span><span class="o">|</span><span class="n">i</span><span class="o">|</span> <span class="n">i</span><span class="o">.</span><span class="n">exists?</span> <span class="o">&amp;&amp;</span> <span class="n">i</span><span class="o">.</span><span class="n">elb_health</span><span class="o">[</span><span class="ss">:state</span><span class="o">]</span> <span class="o">==</span> <span class="s2">&quot;InService&quot;</span><span class="p">}</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="o">&amp;</span><span class="ss">:public_dns_name</span><span class="p">)</span>
</span><span class='line'><span class="n">servers</span> <span class="o">=</span> <span class="n">instances</span> <span class="o">&lt;&lt;</span> <span class="s2">&quot;localhost&quot;</span>
</span><span class='line'><span class="n">role</span> <span class="ss">:app</span> <span class="k">do</span> <span class="n">servers</span> <span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<h2>注意点</h2>

<ul>
<li>同期するファイル数が多すぎる（数十万個）と、チェックサム比較で時間がかかってしまい同期が終了する前にAuto Scalingの死活監視によってインスタンスがterminateされ、またlaunchされterminateされるという恐怖のスケールループ地獄に陥る。。。（これ、制限できないのかな）</li>
<li>あらかじめトラフィック増の時間帯が分かっていたら<a href="http://docs.amazonwebservices.com/AutoScaling/latest/DeveloperGuide/scaling_plan.html#schedule_time">Scheduled Action</a>で多めに設定しておいて自動スケールダウンさせた方が吉。</li>
<li>マスタサーバがSPOFとなりうるので冗長化する、もしくはrsyncよりs3経由にした方が良い。</li>
<li>急激なトラフィック増ではELB自体のスケールが追いつかない場合も。</li>
</ul>

]]></content>
  </entry>
  
</feed>
