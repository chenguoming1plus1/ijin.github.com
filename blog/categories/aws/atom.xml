<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: aws | @ijin]]></title>
  <link href="http://ijin.github.com/blog/categories/aws/atom.xml" rel="self"/>
  <link href="http://ijin.github.com/"/>
  <updated>2013-11-12T15:38:15+09:00</updated>
  <id>http://ijin.github.com/</id>
  <author>
    <name><![CDATA[Michael H. Oshita]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AWS Game Day Tokyo 2013で受賞してきた]]></title>
    <link href="http://ijin.github.com/blog/2013/06/10/aws-game-day-tokyo-2013/"/>
    <updated>2013-06-10T23:33:00+09:00</updated>
    <id>http://ijin.github.com/blog/2013/06/10/aws-game-day-tokyo-2013</id>
    <content type="html"><![CDATA[<p>大統領選挙でオバマ陣営のシステムを堅牢化する為に用いた手法である仮想対戦シミュレーション「<a href="http://gameday2013.doorkeeper.jp/events/3960">AWS Game Day Tokyo</a>」が日本で初めて（世界2番目に）開催されたので参加してきました。</p>

<p>結果、ベスト・ディフェンス賞こと「<strong>Most Awesome Fix!</strong>」賞を受賞しました。</p>

<h2>経緯</h2>

<p>以前、<a href="http://jaws-ug.jp/jawsdays2013/">JAWS DAYS 2013</a>でMiles Wardが講演した「<a href="http://www.publickey1.jp/blog/13/obama_for_america.html">Behind the Scenes of the Presidential Campaign</a>」でチームを攻撃・防御に分けて対戦させ、そこから学んだ事をフィードバックしてシステムをより堅牢にするという「Game Day」を知り、日本でもやりたいねという話になってました。そこで先日の<a href="http://www.awssummittokyo.com/">AWS Summit Tokyo</a>のスピンオフイベントとして、Milesの再来日に合せてADSJ（アマゾンデータサービスジャパン株式会社）さんによって開催される事になりました。チーム戦の大会は<a href="http://ijin.github.io/blog/2012/07/03/tuningathon4/">チューニンガソン</a>や<a href="http://ijin.github.io/blog/2012/11/05/isucon2/">ISUCON</a>以来なので、わくわくしながら速攻で応募をしました。</p>

<h2>概要</h2>

<p>大体、こんな流れです。</p>

<ul>
<li>システムの構築・堅牢化</li>
<li>相手システムの攻撃（この間、自システムも攻撃される）</li>
<li>自システムの修復</li>
<li>評価</li>
</ul>


<p>それぞれ、2〜3人のチームに別れ、計18チームにより対戦。私のチーム名は時事ネタとして今流行りの「<a href="http://jp.techcrunch.com/2013/06/07/20130606report-nsa-collects-data-directly-from-servers-of-google-apple-microsoft-facebook-and-more/">PRISM</a>」としました。
システム概要はnginksさんの<a href="http://d.hatena.ne.jp/nginks/20130608/1370700185">ブログ</a>が非常に分かりやすいです。
要するに画像変換処理バッチクラスターですね。</p>

<h2>構築</h2>

<p>手順書をざっと見ながら画像処理クラスターを構築。相方はシステムの人間ではなかったので、動作確認を手伝ってもらいつつ実質一人でもくもくと作業。s3作成、sqs作成、アプリのインストール・設定・動作確認、AMI化、Cloudwatch設定、AutoScaling設定等を淡々と。構築しながらシステムを把握して行くけど、結構これだけで時間がとられます。なので、じっくりと防衛策は練れなかったのでひとまず、プロセスの自動復旧をしてくれるmonitをインストール・設定し（upstartでやりかけたけどうまく動かなかった）、主要ファイルのchecksumを取って改善検知してメール通知する仕組みを導入。</p>

<h2>攻撃</h2>

<p>AWSキー（Poweruser権限）を奪取したという仮定の元、相手システムに攻撃をしかけるターン。
単純に全部消したり、セキュリティグループの権限を変えたりではあまりにもつまらないので、いろいろ考えます。（無論、システムの全消しは誰でもできる最低の攻撃手法）</p>

<p>まず、状況把握する為にいろいろ動作確認。キューに画像を突っ込んで、ちゃんと処理されるとか。あれ、でも動かない。。
どうやらTokyoで作りかけたけど、結局Virginiaリージョンで仕上げたと運営側から伝えられる。いきなりのタイムロス！</p>

<p>気を取り直して、稼働中のインスタンスに入る方法を思いつく。通常はキー設定されているのでsshでは入れないので仮のインスタンスを起動し旧インスタンスのroot volumeのEBSを強制detachし、仮インスタンスにattachして中身をいじってからre-attachする事に。見た目は同じinstance-idなのに中身だけ違う、一見すると分かりづらいです。そこで旧インスタンスを一旦停止させる為にstopさせると、、terminateされちゃいました。。よくよく調べて見ると、Auto Scalingの設定になっていて、min-sizeの制約によって旧インスタンスが消され、代替インスタンスが自動的に起動するようになってました。</p>

<p>どうやら構築が間に合わなかったチーム用に運営側が用意した自動構築をしてくれる虎の子のCloudformationを使った模様。
そこで、相手チームのスキルレベルがそれ程高くないと判断し、Auto Scalingの元AMIを置き換える事に。
新たなlaunch configを作成し、既存のscaling groupと同盟のものを作成。</p>

<p>次にs3への攻撃。bucket名はglobalなnamespaceなので、こいつを削除して同名のを別AWSアカウントで作れば乗っ取りが可能。。
のはずが、削除してから一定時間経過しないと作成不可だったので1字違いのbucketを作成しておく。</p>

<p>最後にs3のbucket一覧を取得して、常に空のディレクトリと同期し続ける攻撃を思いつき、実装を始める。システムはs3に出力するのでそこを継続的に空にする攻撃です。しかし、実装を初めて動作確認の途中で時間切れになりシステムに埋め込めなくて断念。もうちょっと時間が欲しかったです</p>

<h2>修復</h2>

<p>次は自システムが受けた攻撃を修復するターン。</p>

<p>いろいろ余計なインスタンスが起動していたが、まずやったのがAMI番号の確認。（これが無事であればOSに侵入されていようがAMIをベースに全体の再構築が速やかにできるので）
幸い、控えていたのと一致していたので他のインスタンスを全て停止。一応monitのアラートメールが飛んでいなかったので、インスタンスに対しての操作は限定されているのだろうとは踏んでましたが。</p>

<p>AMIが無事なら次はAuto Scalingの確認。ざっと見た感じだと、lauch configは操作されておらず、scaling groupのmin-sizeが0に変更されている模様。他の変更点は確認が面倒だったので、一旦全部削除してさくっと再作成。後で聞いた話だと、Auto Scalingのrecurring schedule設定で1分起きに0台にする設定をしていたらしいが、消された時点で攻撃は無効化。</p>

<p>次にSQS。消して再作成すればてっとり早いけど、相手チームがキューに投入した画像を最終的に表示させないといけないルールだと誤解していて、その復旧に務める。新しく作ったSQSと比較するとパラメータ（Default Visibility Timeout, Retention Period, Message Size等）が異常な値に変更されていると分かり、通常の値へ戻す。</p>

<p>この時点でアプリとSQSの通信を確認するも疎通できない事を把握。pingが通らない事からSecurity Group, Routing Table, Network ACLが変更されていないかを確認。どうやらSecurity GroupのIn/Outルールが削除されている単純な攻撃だと判明し、なんなく再設定。</p>

<p>キュー内のメッセージが1コ処理されるのを確認し、SQS周りは対応済みかと思ったけど残り2コのメッセージがいくら待てども処理されず、若干悩む。
ログを見たり、メッセージの中身を覗くとと「<strong>--max-redirect=99999999</strong> 」が目に留まる。どうやら変換する画像をダウンロードする部分で無限ループさせている模様。メッセージを削除し、そのパラメータを除外したものを流してちゃんとキューが処理される事を確認。</p>

<p>最後にs3周りで怪しい設定がないかを調査して、一通りの動作確認をして復旧完了。</p>

<h2>振り返り</h2>

<p>お互いに対戦したチームと顔合わせをし、攻撃や復旧の手の内を明かします。全チームの行動記録を集約して運営側で審査を行われ各賞が授与される中、我がPRISMはSQS内の無限ループを検知・修復したのが評価されて最も優れた修復を行った「<strong>Most Awesome Fix!賞</strong>」を頂きました。後で他のチームに聞いた所、monitのような検知・通知の仕組みを導入した所はなさそうだったので、それも評価ポイントだったかも知れません。</p>

<p>賞の内容としては、ワンタイムトークンを生成するハードウェアMFAデバイスとAWSのクーポンコードでした。ありがとうございます。</p>

<p>最後にMilesが壊れても戻せるようにあるべき状態の定義と常に比較して自動的に自己治癒するのが最高のシステムと言ってました。AWSの状態を保存するにはCloudformerでCloudformationテンプレート化すれば便利で楽だけど、chefみたいにIdempotency（冪等性）を継続的に保証する仕組みをそのレイヤーで組むのはなかなか大変ですね。（個々のサーバ単位は可能だとしても）</p>

<h2>感想</h2>

<p>「<em>チューニンガソン</em>」や私が他にお手伝いをしている「<em>トラブル☆しゅーたーず</em>」とも一味違って、非常に楽しめました。</p>

<p>以下、思った事をいくつか。</p>

<ul>
<li>構築に時間が取られたのでなるべく出来合いのシステムがあった方が防衛策に専念できそう</li>
<li>攻撃可能な時間が思ったより短かったのでもうちょっと長めで</li>
<li>Default-VPCとEC2-Classicでは挙動が違うのでアカウントタイプは統一した方が良い</li>
<li>ターン性ではなく、攻撃と修復・防御のリアルタイム性を試すとか</li>
<li>あるべき正しい状態を把握する為のツールがあると他のタスクに集中できるかも</li>
</ul>


<p>次に開催される時も参加したいですね。もしくは運営側のお手伝いでも！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Non-RDSなオレオレMulti-AZ MySQL Replication]]></title>
    <link href="http://ijin.github.com/blog/2013/05/21/custom-non-rds-multi-az-mysql-replication/"/>
    <updated>2013-05-21T18:03:00+09:00</updated>
    <id>http://ijin.github.com/blog/2013/05/21/custom-non-rds-multi-az-mysql-replication</id>
    <content type="html"><![CDATA[<p>先日（5/17/13）<a href="http://www.zusaar.com/event/668006">cloudpack night #6 - Re:Generate -</a>に参加してきました。</p>

<p>当日の様子はcloudpack<a href="https://twitter.com/yoshidashingo">吉田</a>さんの以下のレポートで。<br />
<a href="http://d.hatena.ne.jp/yoshidashingo/20130518/1368853720">cloudpack Night #6 - re:Generate - を開催しました</a></p>

<p>私はDJを少々とLTで参加させて頂いたのでその内容になります。</p>

<h2>オレオレMulti-AZのススメ</h2>

<p>AWS上でMySQLを使う場合、RDSはてっとり早くて良いんですが、たまにもうちょっと柔軟性が欲しい時があります。
例えば別のストーレジエンジンやディストリビューションを使ったり（Percona Server, MariaDB, TokuDB, Mronnga等）、RDSでは使えないインスタンスファミリー（hi1, m3, c1系）を使ったり、OSレベルでのチューニングができたり、スレーブのバッファプールを予め温めておいたり。等々。</p>

<p>しかし、RDSにはAvailability Zone (AZ)をまたいでフェールオーバーするMulti-AZ機能があり、AWSで設計するにはAZ障害を考慮した方が推奨されます。</p>

<p>そこで、MHAとVPCを組み合わせて柔軟性をもったMulti-AZ環境を実現します。
（ちなみに私の場合はhi1.4xlargeでPercona Serverを冗長化をする必要があったから）</p>

<h3>MHA</h3>

<p><a href="https://code.google.com/p/mysql-master-ha/">MHA</a>とはFacebookの<a href="http://yoshinorimatsunobu.blogspot.jp/">松信</a>さんがDeNA時代に作ったMySQLの自動フェールオーバーしてくれうナイスなツールです。Master障害時にbinlog同期とSlaveの昇格を全自動でやってくれます。昇格時にはカタログデータベースに更新をかけて新DB構成のIPの情報を更新するか、Virtual IPを切り替えるかをする必要（この担当部分はmaster_ip_failover_scriptで対応）があるけど、今回は後者的なアプローチになります。</p>

<h3>VPC</h3>

<p>ENIを使えばVirtual IP的な使い方はできるけどAZは超えられないので、source/dest. checkを無効化した上でrouting tableによって擬似Virtual IPを実現する<a href="http://aws.clouddesignpattern.org/index.php/CDP:Routing-Based_HA%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3">Routing-Based HAパターン</a>（CDP 2.0候補）を使います。ADSJ<a href="https://twitter.com/c9katayama">片山</a>さんの<a href="http://d.hatena.ne.jp/c9katayama/20111225/1324837509">エントリ</a>が発端ですね。</p>

<h2>Demo</h2>

<p>以下、LT時に見せたデモ動画</p>

<p><div class="embed-video-container"><iframe src="http://www.youtube.com/embed/tovb29K6ddc "></iframe></div></p>

<p>擬似Virtual IPに対してそれぞれread/writeを行いつつMasterのプロセスをkillすると、通信できなくエラーが出続けるが20秒以内に自動フェールオーバーが完了しread/write共に再開します。pingも平均0.5msから2.0msに変わった事からAZが移った事が確認できます。</p>

<p>RDSのMulti-AZの場合、フェールオーバーには3-6分かかるので、かなり短時間で復旧が可能です。
また、RDSはCNAME切替によるDNSベースに対して、MHA+VPC構成の場合はIP指定することができます。そうするとアプリからのresolveが不要になり、去年起こった内部DNSが引けない障害が起きても問題ありません。</p>

<h2>注意点</h2>

<ul>
<li>RDSな自動バックアップがない

<ul>
<li>Xtrabackupとbinlogの定期s3保存で対応</li>
</ul>
</li>
<li>Point in Timeリカバリー

<ul>
<li>Chef等で自動化しましょう</li>
</ul>
</li>
<li>Read Replicaの作成

<ul>
<li>Chefで頑張りましょう</li>
</ul>
</li>
<li>学習曲線

<ul>
<li>勉強しましょう</li>
</ul>
</li>
<li>API backplaneがSPoF

<ul>
<li>AWSに祈りましょう</li>
</ul>
</li>
</ul>


<p>特に一番の懸念点は最後のAPI backplane。AWSの今までの大規模障害状況を見ていると、皆が同時に復旧をしようとしAPIへのリクエストが大量に集中してそこがボトルネックになり、リソースの操作不能に陥るという悲惨な事象が何回かありました。まあ、その場合はRDSでも同じような気はしますが、ここは当時の障害を経験にキャパシティが増加されている事を信じておくしかありませんね。。</p>

<h2>おわりに</h2>

<p>とまあ、こんなLTをしたわけですが、この後に続いたCookpadの<a href="https://twitter.com/sgwr_dts">菅原</a>さんの<a href="http://www.slideshare.net/winebarrel/ec2keepalivedlvsdsr">LT</a>の方が盛り上がって自分のは余興に終わってしまいました。</p>

<p>あ、ついでにその日はcpniteの資料作成やDJの選曲であんまり寝てなかったにもかかわらず、無事AWSソリューションアーキテクト（Associate）の認定試験に受かりました！</p>

<p><img src="https://lh5.googleusercontent.com/-d_HVsb6DgBc/UZs39XB1KGI/AAAAAAAAAuw/ntgPp33hcOI/w294-h120-no/Solutions-Architect-Associate.png"></p>

<h2>LTスライド</h2>

<p><div class="embed-ss-container"><iframe src="http://www.slideshare.net/slideshow/embed_code/21341276 "></iframe></div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[リージョン間高速データ転送]]></title>
    <link href="http://ijin.github.com/blog/2013/04/03/accelerating-cross-region-data-transfer/"/>
    <updated>2013-04-03T09:05:00+09:00</updated>
    <id>http://ijin.github.com/blog/2013/04/03/accelerating-cross-region-data-transfer</id>
    <content type="html"><![CDATA[<p>先日の<a href="http://jaws-ug.jp/jawsdays2013/">JAWS DAYS</a>のセッション<a href="http://www.publickey1.jp/blog/13/obama_for_america.html">「Behind the scenes of Presidential Campaign」</a>でリージョン間のデータ転送を高速化するツールとして<a href="http://tsunami-udp.sourceforge.net">tsunami udp</a>と<a href="http://www.cloudopt.com">cloudopt</a>を使った話が出てたので試してみました。</p>

<p>通常、遠距離のサーバはRTTが大きくなるのでスループットが下がり、巨大なデータ転送には苦労しますが、なんと27TBを9時間で転送したとのこと！実際はマシンを並列にして同時転送したらしいので1台での実験です。</p>

<h3>前提</h3>

<ul>
<li>東京(server1) -> アメリカ西海岸(server2)</li>
<li>RTT: 120msぐらい</li>
<li>EC2 instance type: m1.large</li>
<li>OS: Ubuntu 12.04</li>
</ul>


<h3>ベーステスト</h3>

<p>10Gファイルの作成</p>

<pre><code>server1$ mkdir _tmp; cd _tmp
server1$ dd if=/dev/zero of=10G count=1 bs=1 seek=10G
</code></pre>

<p>転送</p>

<pre><code>server1$ scp -rp 10G server2:
10G     100%   10GB  11.1MB/s   15:22
</code></pre>

<p>だいたい、90Mbpsぐらい。</p>

<h3>Tsunami UDP</h3>

<p>インストールは両サーバで</p>

<pre><code>sudo apt-get install make gcc autoconf
wget http://downloads.sourceforge.net/project/tsunami-udp/tsunami-udp/tsunami-v1.1-cvsbuild42/tsunami-v1.1-cvsbuild42.tar.gz
tar xvfz tsunami-v1.1-cvsbuild42.tar.gz
cd tsunami-udp-v11-b42
make
sudo make install
</code></pre>

<p>tsunami udpはfetch型の作りなので、送信側のサーバ（server1）で対象ファイルのあるディレクトリに移動してサービス起動</p>

<pre><code>server1$ cd _tmp
server1$ tsunamid *
</code></pre>

<p>で、クライアント(server2)からファイルをfetch</p>

<pre><code>server2$ tsunami connect ec2-xx-x-xx-x-x get 10G quit
</code></pre>

<p>本当はftp-likeは対話型クライアントだけど、ワンライナーでも可能。  <br/>
転送が完了するとクライアント側で情報が出力されます。</p>

<pre><code>Transfer complete. Flushing to disk and signaling server to stop...
!!!!
PC performance figure : 224947 packets dropped (if high this indicates receiving PC overload)
Transfer duration     : 419.32 seconds
Total packet data     : 183339.18 Mbit
Goodput data          : 181958.17 Mbit
File data             : 81920.00 Mbit
Throughput            : 437.23 Mbps
Goodput w/ restarts   : 433.93 Mbps
Final file rate       : 195.36 Mbps
Transfer mode         : lossless
</code></pre>

<p>おお、確かに速い！スループットもセッションでも言ってた476Mbpsに近いし。</p>

<p>サーバ側では</p>

<pre><code>Server 1 transferred 10737418241 bytes in 419.33 seconds (195.4 Mbps)
</code></pre>

<p>ファイル自身の実際の転送レートはFinal file rateである<strong>195.4Mbps</strong>。
多分、パラメータチューニングやより速いディスクを使うとスループットはさらに向上すると予想。</p>

<h3>cloudopt</h3>

<p>次は圧縮、TCP最適化、data deduplication等、様々な技術を用いてWANを高速化する<a href="http://www.cloudopt.com">cloudopt</a>の実験。こちらは有料で、ソフトウェアをインストールしてライセンスを購入（15日間のお試しあり）して使う方法とAmazon Marketplaceでセットアップ済みの課金型AMIを起動する方法がとれます。今回はてっとり早く後者で。</p>

<p>AMIはMarketplaceでCloudoptを検索し、各リージョンで1台づつ起動。</p>

<p><img src="https://lh5.googleusercontent.com/-7PWSb_8ClIo/UVuL_bzBeAI/AAAAAAAAAtc/8Lw_bDnaPVE/s665/cloudopt_marketplace_+2013-03-30+at+2.08.07+PM.png"></p>

<p>Ubuntuベースなのが良いですね。</p>

<p>まずscpから呼べるcloudcopyを使っての転送</p>

<pre><code>server1$ scp -rp -S cloudcopy 10G server2:
10G                 100%   10GB  41.5MB/s   04:07     
CloudCopy transferred 17.22 MB, saving 99.8% of bandwidth by sending 9.983 fewer GB 
</code></pre>

<p>お、速い。しかしよく見てみると17.22MBしか転送されてません。どうやらファイル自体が全てゼロ埋めなので圧縮とdeduplicationが最大限効いているみたい。</p>

<p>では、今度は実際のDBのバックアップで転送実験（容量14GB）</p>

<pre><code>server1$ scp -rp -S cloudcopy dbbackup.tar server2:
dbbackup.tar                100%   14GB  11.2MB/s   20:42   
CloudCopy transferred 6.041 GB, saving 55.5% of bandwidth by sending 7.528 fewer GB
</code></pre>

<p>スループットはほぼ一緒だけど、転送容量が削減できてます。</p>

<p>一度転送したファイルはbyte cacheされるので、次に転送する時は差分だけ送るので高速化されます。</p>

<pre><code>server2$ rm dbbackup.tar 

server1$ tar rvf dbbackup.tar file
server1$ scp -rp -S cloudcopy dbbackup.tar server2:
dbbackup.tar                100%   14GB  20.0MB/s   11:35  
CloudCopy transferred 59.57 MB, saving 99.6% of bandwidth by sending 13.511 fewer GB
</code></pre>

<h3>リージョン間レプリケーション</h3>

<p>次はMySQLのレプリケーション実験。</p>

<p>各インスタンスでのピア設定</p>

<pre><code>server1$ sudo cloudconfig peer_add server2 server2_local_ip(10.x.x.x)
server2$ sudo cloudconfig peer_add server1 server1_local_ip(10.x.x.x)
</code></pre>

<p>これでcloudoptを使ったサーバ間のトンネルが確立されます。MySQLのレプリケーションはprivate ipでの設定が必要。
一通りレプリケーションが出来き、スレーブIOを停止した状態でマスターにしばらく書き込んだのちに再開すると、binlogが転送されるのでcloudstatsコマンドで統計が見れます。</p>

<pre><code>Component - cloudoptimizer
------------------------------------------------------------
           Number of connections:                    1
              Original data size:            111.96 MB
           Transferred data size:             52.44 MB

            Bandwidth Saving:             59.51 MB (53.2%)
</code></pre>

<p><strong>53.2%</strong>の転送容量削減！</p>

<h3>結論</h3>

<p>以上を組み合わせれば、新たなCDP候補である「<strong>リージョン間レプリケーションパターン (Cross-Region Replication Pattern)</strong>」が実現できます。</p>

<ul>
<li>巨大データをリージョン間で転送するにはtsunami udpが有効そう</li>
<li>リージョン間での差分バックアップやレプリケーション向けにはcloudoptで高速化</li>
<li>普通のHTTP通信とかも使えるかも</li>
<li>s3へのアップロード高速化も対応しているのでいずれ試してみたい</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS SSD (hi1.4xlarge) vs Fusion-IOでのMySQLベンチマーク]]></title>
    <link href="http://ijin.github.com/blog/2013/02/22/mysql-benchmarks-on-aws-ssd-vs-fusion-io/"/>
    <updated>2013-02-22T17:32:00+09:00</updated>
    <id>http://ijin.github.com/blog/2013/02/22/mysql-benchmarks-on-aws-ssd-vs-fusion-io</id>
    <content type="html"><![CDATA[<p>（※ <a href="#add">追記</a>しました - 5/19/13）</p>

<p>巷ではMySQL 5.6 GAが出て騒がしいですが、ちょっと前に5.5系でAWSのSSDインスタンス（hi1.4xlarge）に載せ替える案件があったので、その時に取ったベンチマークを公表します。以前Fusion-IO (ioDrive Duo)でも同じようにやったので、比較になれば。</p>

<h2>経緯</h2>

<ul>
<li>あるウェブサービスのDBサイズが巨大でm2.4xlargeでも辛くなってきている</li>
<li>アクセスパターンによりパーティショニングが効かない</li>
<li>シャーディングをするにはアプリ改修が大変</li>
<li>数週間後に急激なアクセスが予想され、時間的余裕がない！</li>
<li>データサイズの急激な増加によりbuffer poolから溢れ、ディスクアクセスのさらなる発生が懸念</li>
</ul>


<p>というわけで、時間がないのでSSDへの移行を検討し、ベンチマークを取りました。</p>

<h2>ベンチマーク</h2>

<p>buffer poolが徐々に足りなくなった場合のディスクアクセスの発生をシミュレート</p>

<ul>
<li>sysbenchのoltpモード</li>
<li>データサイズは12G（5000万件）</li>
<li>readonly</li>
<li>uniform（フルスキャン）</li>
</ul>


<p>主要my.cnfパラメータ</p>

<pre><code> sync_binlog = 0
 innodb_buffer_pool_size = XXG
 innodb_flush_method = O_DIRECT
 innodb_flush_log_at_trx_commit = 1
 innodb_file_per_table
 innodb_io_capacity = 2000
</code></pre>

<p>コマンド</p>

<pre><code> time sysbench --test=oltp --oltp-table-size=50000000 --db-driver=mysql --mysql-user=root prepare                                                                                                         
 time sysbench --test=oltp --oltp-table-size=50000000 --db-driver=mysql --mysql-user=root --num-threads=16 --max-requests=0 --max-time=180 --init-rng=on --oltp-read-only=on --oltp-dist-type=uniform 2&gt;&amp;1 run                                                                                                         
</code></pre>

<h2>結果</h2>

<p>トランザクション推移</p>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&amp;oid=2&amp;zx=ii4lryrf8pz8"></p>

<p>レスポンスタイム推移</p>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&amp;oid=3&amp;zx=c2drap7b5561"></p>

<p>Fusion-IOと比べて半分ぐらい。ioDrvie Duoの公称IOPSが250,000+ IOPSでhi1.4xlargeが120,000 IOPSなので、まあ合致しますね。</p>

<p>まだ整理されてないベンチマーク結果があるので、後ほど追記しようと思います。</p>

<h2>追記 <a name="add">&nbsp;</a></h2>

<p>先日（5/17/13）の<a href="http://www.zusaar.com/event/668006">cloudpack night #6 - Re:Generate -</a>でADSJの荒木さんの発表でMySQLのパフォーマンスの話があったので<a href="https://code.launchpad.net/~percona-dev/perconatools/tpcc-mysql">tpcc-mysql</a>でベンチマークを取った資料を思い出し追記しました。</p>

<ul>
<li>500 warehouses (50GBぐらい)</li>
<li>24GB Buffer pool</li>
<li>16スレッド</li>
<li>1時間実行</li>
</ul>


<p>コマンド（ロードはかなり時間がかかるので注意）</p>

<pre><code> tpcc_load localhost tpcc root "" 500
 tpcc_start -d tpcc -u root -p "" -w 500 -c 16 -r 300 -l 3600
</code></pre>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&amp;oid=4&amp;zx=y6kwyezb1qth"></p>

<p>hi1.4xlargeの方が安定するまで少し時間がかかってます。<br />
以下、安定化しだした900sあたりからの数値です。</p>

<table>
<thead>
<tr>
<th></th>
<th>Fusion-IO</th>
<th>hi1.4xlarge</th>
</tr>
</thead>
<tbody>
<tr>
<td>total</td>
<td>2288758</td>
<td>1444417</td>
</tr>
<tr>
<td>avg</td>
<td>8445.6</td>
<td>5330.0</td>
</tr>
<tr>
<td>stddev</td>
<td>245.7</td>
<td>132.8</td>
</tr>
</tbody>
</table>


<p>Fusion-IOに比べて6割ってところでしょうか。
今回はSSDのephemeral disk1本で試したので、RAID0にするともうちょっと違ってくると思います。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[非ELBなAutoscalingによる自動復旧]]></title>
    <link href="http://ijin.github.com/blog/2013/02/08/self-healing-with-non-elb-autoscaling/"/>
    <updated>2013-02-08T09:29:00+09:00</updated>
    <id>http://ijin.github.com/blog/2013/02/08/self-healing-with-non-elb-autoscaling</id>
    <content type="html"><![CDATA[<p>バッチ処理等、サーバの冗長化が難しく仕方なく1台で動かさざるを得ない場合があります。でも可用性は確保したい。また、Pacemakerやkeepalived等できなくはないが、お金もあんまりかけられない場合もあります。
そんな時にAWS上でよく使うのがAutoscalingによる1台構成です。最低台数・最大台数共に「1」に設定しておけばEC2インスタンスが壊れても自動的に新しいのにリプレースされて復旧されます。</p>

<p>しかし、Autoscalingのhealth-check-typeを「EC2」にした場合、インスタンスの起動状態（running, stopped）しか見てくれないので、今までこの構成を実現するのにELBによる死活監視が必要でした。インスタンスがHTTPサーバじゃない場合、ちょっとムダです。</p>

<p>ところが、ちょっと前にAutoscalingがインスタンスの健康状態をチェックするEC2 status checkに<a href="http://aws.amazon.com/about-aws/whats-new/2012/12/14/auto-scaling-now-uses-amazon-ec2-status-checks/">対応</a>し、ELBが不要になったはずなので試してみました。</p>

<p>今回は各種AWSサービスに対応した統合されたPython版の<a href="http://aws.amazon.com/cli/">AWS CLI</a>ツールを使います。</p>

<h3>セットアップ</h3>

<p>まずは、ツールのインストール</p>

<pre><code>sudo apt-get install python-pip
sudo pip install awscli
complete -C aws_completer aws
</code></pre>

<h3>Autoscaling設定</h3>

<p>Launch Configの設定</p>

<pre><code>aws autoscaling create-launch-configuration --image-id ami-4a12aa4b \
--launch-configuration-name test-lc --instance-type t1.micro --key-name ijin-tokyo \
--security-groups test --iam-instance-profile test_iam

{
    "ResponseMetadata": {
        "RequestId": "c0e66974-7103-11e2-9780-a53199bac60e"
    }
}
</code></pre>

<p>Scaling Groupの設定</p>

<pre><code>aws autoscaling create-auto-scaling-group --auto-scaling-group-name test-sg \
--launch-configuration-name test-lc --min-size 1 --max-size 1 \
--health-check-grace-period 180 --tags '{"key":"Name", "value":"as-test"}' \
'{"key":"Use Case", "value":"test"}' --availability-zones ap-northeast-1a --health-check-type "EC2"

{
    "ResponseMetadata": {
        "RequestId": "e3808ef3-7103-11e2-9780-a53199bac60e"
    }
}
</code></pre>

<p>通知</p>

<pre><code>aws autoscaling put-notification-configuration --auto-scaling-group-name test-sg \
--topic-arn arn:aws:sns:ap-northeast-1:521026608000:test \
--notification-types autoscaling:EC2_INSTANCE_LAUNCH autoscaling:EC2_INSTANCE_TERMINATE \
autoscaling:EC2_INSTANCE_LAUNCH_ERROR autoscaling:EC2_INSTANCE_TERMINATE_ERROR

{
    "ResponseMetadata": {
        "RequestId": "f68359be-7103-11e2-9a1a-5f77b12b596e"
    }
}
</code></pre>

<p>レスポンスがjsonなのが良いですね。
また、<a href="http://aws.amazon.com/developertools/2535">Auto Scaling Command Line Tool</a>と違って、tagでスペースが使えるようになったのが素晴らしい！</p>

<p>以上の設定でAutoscalingによってインスタンスが1台立ち上がります。</p>

<h3>自動復旧</h3>

<p>最後にインスタンス不調（status check failure）をシミュレートする為にインスタンス内からネットワークを落とします。</p>

<pre><code>ubuntu@ip-10-128-25-25:~$ sudo ifdown eth0
</code></pre>

<p>これでstatus checkがfailし、Autoscalingが自動的に新しいインスタンスと取り替えてくれるはず！</p>

<p><img src="https://lh6.googleusercontent.com/-9FpM6ywjg84/URN6q29FD-I/AAAAAAAAAsw/paP8HBnisPE/s161/aws_status_check_2013-02-07+18.06.20.png"></p>

<p><img src="https://lh5.googleusercontent.com/-5avxLvp5RH8/URN6n-ihwiI/AAAAAAAAAso/zOHxli4vM8o/s702/aws_instance_check_2013-02-07+18.09.19.png"></p>

<p>と、期待して待っていたらなかなかアラートメールが来ません。。</p>

<p>設定間違ったかなーっていろいろ見直していたら20分経った頃にやっと到着。</p>

<p><code>
Service: AWS Auto Scaling
Time: 2013-02-07T09:17:42.304Z
RequestId: f395660b-4847-4415-ad33-f8cc5091bdb3
Event: autoscaling:EC2_INSTANCE_TERMINATE
AccountId: 521026608000
AutoScalingGroupName: test-sg
AutoScalingGroupARN: arn:aws:autoscaling:ap-northeast-1:521026608000:autoScalingGroup:a036877b-dab7-4e5d-a6e1-1d3424b20d14:autoScalingGroupName/test-sg
ActivityId: f395660b-4837-4415-ad33-f8cc5071bdb3
Description: Terminating EC2 instance: i-15fadd17
Cause: At 2013-02-07T09:16:57Z an instance was taken out of service in response to a system health-check.
StartTime: 2013-02-07T09:16:57.389Z
EndTime: 2013-02-07T09:17:42.304Z
StatusCode: InProgress
StatusMessage:
Progress: 50
EC2InstanceId: i-15fadd17
Details: {}
</code></p>

<p>うーん。動く事は動いたけど、ちょっと時間がかかるなぁ。</p>

<p>この後何回か試してみたけど、Autoscaling発動までどれも20分ぐらいかかりました。</p>

<h3>結論</h3>

<p>20分程サーバダウンが許容できるようなゆるめの条件に限定した場合には非ELBでも使えるかな。まあ、それでも適用する場面は多々あるとは思いますが。（早める方法あるのかなー。）</p>
]]></content>
  </entry>
  
</feed>
